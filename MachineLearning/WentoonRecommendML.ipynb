{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPnnu+ucg3RlIZhIIrSlYrF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0FHWcwIktk0","executionInfo":{"status":"ok","timestamp":1667832717666,"user_tz":-540,"elapsed":21787,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"9e3552fb-cc56-4a92-9bb3-59eebf076249"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# download library"],"metadata":{"id":"EYpGyNd1Rzs5"}},{"cell_type":"code","source":["!pip install selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","\n","!pip install konlpy\n","\n","!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"],"metadata":{"id":"YZUjKbJaR2q3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# myUtil"],"metadata":{"id":"Gb88r205K0AE"}},{"cell_type":"code","source":["import pandas as pd\n","import pickle as pk\n","from urllib import request\n","from PIL import Image\n","import os\n","\n","\n","class ut:\n","    # 모아둔 정보로 CSV 파일 생성하기\n","    @staticmethod\n","    def make_csv(filename, ti):\n","        ti.to_csv(filename, encoding='utf-8-sig')\n","\n","    # CSV 파일이 있다면 가져오기\n","    @staticmethod\n","    def get_from_csv(filename):\n","        return pd.read_csv(filename, encoding='utf-8-sig')\n","\n","    # CSV 파일을 지우기\n","    @staticmethod\n","    def delete_csv(filename):\n","        if os.path.isfile(filename):\n","            os.path.isdir\n","            os.remove(filename)\n","\n","    # 특정 데이터를 파일에 저장해놓기\n","    @staticmethod\n","    def save_data(filename, data):\n","        with open(filename, 'wb') as f:\n","            pk.dump(data, f)\n","\n","    # 저장해둔 데이터 불러오기\n","    @staticmethod\n","    def load_data(filename):\n","        with open(filename, 'rb') as f:\n","            return pk.load(f)\n","\n","    @staticmethod\n","    def make_data_folder():\n","        if not os.path.isdir(\"data\"):\n","            os.makedirs(\"data\")\n","\n","    @staticmethod\n","    def save_images(urls):\n","        if not os.path.isdir(\"data/images\"):\n","            os.makedirs(\"data/images\")\n","            os.makedirs(\"data/resized_images\")\n","            print(\"--images downloading start--\")\n","            for idx, url in enumerate(urls):\n","                print(\"\\r\" + str(idx + 1) + \"/\" + str(len(urls)), end=\"\")\n","                img_name = \"data/images/thumbnail\" + str(idx) + \".jpg\"\n","                resized_img_name = \"data/resized_images/thumbnail\" + str(idx) + \".jpeg\"\n","                request.urlretrieve(url, img_name)\n","                img = Image.open(img_name).convert('RGB')\n","                img.save(resized_img_name, 'JPEG', qualty=85)\n","\n","            print()\n","            print(\"--images downloading end--\")\n","        else:\n","            print(\"--images already exist--\")\n"],"metadata":{"id":"YOU4KgmzLJxr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# totalData"],"metadata":{"id":"MdEW3OqxK48_"}},{"cell_type":"code","source":["import pandas as pd\n","\n","\n","class td:\n","    # 전체 정보를 저장할 변수\n","    total_data = pd.DataFrame({\n","        \"id\": [],\n","        \"cluster_story\": [],\n","        \"cluster_story_in_genre\": [],\n","        \"cluster_style\": [],\n","        \"cluster_story_group\": [],\n","        \"cluster_story_group_in_genre\": [],\n","        \"cluster_style_group\": [],\n","        \"thumbnail\": [],\n","        \"title\": [],\n","        \"author\": [],\n","        \"day\": [],\n","        \"genre\": [],\n","        \"story\": [],\n","        \"platform\": [],\n","        \"url\": [],\n","    })\n","\n","    # 각 클러스터별 핵심 단어를 저장할 변수\n","    cluster_details = pd.DataFrame({\n","        \"genre\": [],\n","        \"cluster_num\": [],\n","        \"words\": [],\n","    })\n","\n","    # 카테고리 목록\n","    categories = []\n","\n","    @staticmethod\n","    def make_total_data(wd):\n","        my_total_data = pd.DataFrame({\n","            \"id\": [],\n","            \"thumbnail\": [],\n","            \"title\": [],\n","            \"author\": [],\n","            \"day\": [],\n","            \"genre\": [],\n","            \"story\": [],\n","            \"platform\": [],\n","            \"url\": [],\n","        })\n","\n","        my_total_data['id'] = wd.id_list\n","        my_total_data['thumbnail'] = wd.thumbnail_list\n","        my_total_data['title'] = wd.title_list\n","        my_total_data['author'] = wd.author_list\n","        my_total_data['day'] = wd.day_list\n","        my_total_data['genre'] = wd.genre_list\n","        my_total_data['story'] = wd.story_list\n","        my_total_data['platform'] = wd.platform_list\n","        my_total_data['url'] = wd.url_list\n","\n","        my_total_data = my_total_data.drop_duplicates(['title'])\n","        my_total_data.set_index('id', inplace=True)\n","\n","        return my_total_data\n","\n","    @staticmethod\n","    def merge_total_data(tds):\n","        td.total_data = pd.concat(tds)\n","        first_td_len = len(tds[0])\n","        td.total_data['id'] = [i for i in range(len(td.total_data))]\n","        td.total_data.set_index('id', inplace=True)\n","        td.total_data = td.total_data.loc[:, ~td.total_data.columns.str.contains('^Unnamed')]\n","\n","    @staticmethod\n","    def save_category():\n","        td.categories = list(set(td.total_data['genre']))\n","        print(\"\\n<웹툰 카테고리 종류 및 개수>\")\n","        print(\"전체: \" + str(len(td.total_data)))\n","        for genre in td.categories:\n","            print(genre + \": \" + str(len(td.total_data.index[td.total_data['genre'] == genre])))\n","        print()\n","\n"],"metadata":{"id":"xn7QTjb2LNSt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# webtoonData"],"metadata":{"id":"G5HF7pziK-BR"}},{"cell_type":"code","source":["class WebtoonData:\n","    def __init__(self):\n","        self.id_list = []\n","        self.thumbnail_list = []\n","        self.title_list = []\n","        self.author_list = []\n","        self.day_list = []\n","        self.genre_list = []\n","        self.story_list = []\n","        self.platform_list = []\n","        self.url_list = []"],"metadata":{"id":"QVzBmmToLRlx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# myWebCrawling"],"metadata":{"id":"Zs8-qTRcKRE6"}},{"cell_type":"code","source":["from selenium import webdriver\n"," \n","#Colab에선 웹브라우저 창이 뜨지 않으므로 별도 설정한다.\n"," \n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless')        # Head-less 설정\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')"],"metadata":{"id":"xT25OUPlN-w3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGiCtPa3IXcA","executionInfo":{"status":"ok","timestamp":1667538330298,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3880f1b8-8daa-42bb-b13b-b632f04aa0b1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n","  RequestsDependencyWarning)\n"]}],"source":["import pandas as pd\n","import requests\n","import datetime\n","from bs4 import BeautifulSoup as bs\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.common.keys import Keys\n","from selenium.webdriver import ActionChains\n","from time import sleep\n","from webtoonData import WebtoonData\n","from totalData import TotalData as td\n","from myUtil import MyUtil as ut\n","import os.path\n","\n","\n","class MyWebCrawling:\n","    nw_url = 'https://comic.naver.com/webtoon/'\n","    kw_url = 'https://page.kakao.com/main?categoryUid=10&subCategoryUid=1002'\n","    chromedriver_url = 'C:/Storage/Storage_Coding/PycharmProjects/SideProjcet-WebtoonRecommendation/Pycharm/chromedriver'\n","    login_id = ''\n","    login_pw = ''\n","\n","    def get_weekday_info(self):\n","        driver = webdriver.Chrome('chromedriver', options=options)\n","        html = requests.get(self.nw_url+\"weekday\").text\n","        soup = bs(html, 'html.parser')\n","        title = soup.find_all('a', {'class': 'title'})\n","        driver.get(self.nw_url+\"weekday\")\n","\n","        part_wd = WebtoonData()\n","        # 각각의 웹툰 정보 수집 시작\n","        idx = 0\n","        for i in range(len(title)):\n","            sleep(0.5)\n","            print(\"\\rprocess(weekday)): \" + str(i + 1) + \" / \" + str(len(title)), end=\"\")\n","            # 월요일 첫 번째 웹툰부터 순서대로 클릭\n","            page = driver.find_elements(By.CLASS_NAME, \"title\")\n","            page[i].click()\n","\n","            # 이동한 페이지 주소 읽고 파싱\n","            html = driver.page_source\n","            soup = bs(html, 'html.parser')\n","\n","            # 요일 수집\n","            day = soup.find_all('ul', {'class': 'category_tab'})\n","            day = day[0].find('li', {'class': 'on'}).text[0:1]\n","\n","            # 요일 두 개 이상이면 요일만 추가함\n","            current_title = title[i].text\n","            if current_title in part_wd.title_list:\n","                part_wd.day_list[part_wd.title_list.index(current_title)] += ', ' + day\n","                driver.back()\n","                continue\n","\n","            # 나머지 정보 수집\n","            image_url = soup.find('div', {'class': 'thumb'}).find('a').find('img')\n","            image_url = image_url['src']\n","            author = soup.find('span', {'class': 'wrt_nm'}).text[8:]\n","            author = author.replace(' / ', ', ')\n","            genre = soup.find('span', {'class': 'genre'}).text.split(\", \")\n","            story = soup.find('div', {'class': 'detail'}).find('p').text\n","\n","            # 리스트에 추가\n","            part_wd.id_list.append(idx)\n","            part_wd.thumbnail_list.append(image_url)\n","            part_wd.title_list.append(current_title)\n","            part_wd.author_list.append(author)\n","            part_wd.day_list.append(day)\n","            if genre[1] == \"무협/사극\":\n","                part_wd.genre_list.append(\"무협\")\n","            else:\n","                part_wd.genre_list.append(genre[1])\n","            part_wd.story_list.append(story)\n","            part_wd.platform_list.append(\"네이버\")\n","            part_wd.url_list.append(driver.current_url)\n","\n","            # 뒤로 가기\n","            idx += 1\n","            driver.back()\n","            sleep(0.5)\n","        return part_wd\n","\n","    def get_finish_info(self):\n","        driver = webdriver.Chrome('chromedriver', options=options)\n","        html = requests.get(self.nw_url + \"finish\").text\n","        soup = bs(html, 'html.parser')\n","        thumb = soup.find_all('div', {'class': 'thumb'})\n","        driver.get(self.nw_url + \"finish\")\n","\n","        part_wd = WebtoonData()\n","        # 웹툰 정보 수집 시작\n","        idx = 0\n","        for i in range(len(thumb)):\n","            sleep(0.5)\n","            print(\"\\rprocess(finish)): \" + str(i + 1) + \" / \" + str(len(thumb)), end=\"\")\n","            # 첫 번째 웹툰부터 순서대로 클릭\n","            page = driver.find_elements(By.CLASS_NAME, \"thumb\")[1:]\n","            page[i].click()\n","\n","            # 이동한 페이지 주소 읽고 파싱\n","            html = driver.page_source\n","            soup = bs(html, 'html.parser')\n","\n","            # 정보 수집\n","            day = \"완결\"\n","            title = soup.find('span', {'class': 'title'}).text\n","            image_url = soup.find('div', {'class': 'thumb'}).find('a').find('img')\n","            image_url = image_url['src']\n","            author = soup.find('span', {'class': 'wrt_nm'}).text[8:]\n","            author = author.replace(' / ', ', ')\n","            genre = soup.find('span', {'class': 'genre'}).text.split(\", \")\n","            story = soup.find('div', {'class': 'detail'}).find('p').text\n","\n","            # 리스트에 추가\n","            part_wd.id_list.append(idx)\n","            part_wd.thumbnail_list.append(image_url)\n","            part_wd.title_list.append(title)\n","            part_wd.author_list.append(author)\n","            part_wd.day_list.append(day)\n","            if genre[1] == \"무협/사극\":\n","                part_wd.genre_list.append(\"무협\")\n","            else:\n","                part_wd.genre_list.append(genre[1])\n","            part_wd.story_list.append(story)\n","            part_wd.platform_list.append(\"네이버\")\n","            part_wd.url_list.append(driver.current_url)\n","\n","            # 뒤로 가기\n","            idx += 1\n","            driver.back()\n","            sleep(0.5)\n","        return part_wd\n","\n","    # 네이버 웹툰 각각의 정보 가져오기\n","    def get_naver_webtoon_info(self):\n","        wd = WebtoonData()\n","\n","        if os.path.isfile(\"naver1.csv\"):\n","            first_td = ut.get_from_csv(\"naver1.csv\")\n","        else:\n","            first_wd = self.get_weekday_info()\n","            first_td = td.make_total_data(first_wd)\n","            ut.make_csv(\"naver1.csv\", first_td)\n","\n","        if os.path.isfile(\"naver2.csv\"):\n","            second_td = ut.get_from_csv(\"naver2.csv\")\n","        else:\n","            second_wd = self.get_finish_info()\n","            second_td = td.make_total_data(second_wd)\n","            ut.make_csv(\"naver2.csv\", second_td)\n","\n","\n","        total_td = pd.concat([first_td, second_td])\n","        total_td['id'] = [i for i in range(len(total_td))]\n","        total_td.set_index('id', inplace=True)\n","\n","        print()\n","        return total_td\n","\n","    # 카카오 웹툰 각각의 정보 가져오고 파일로까지 저장하기 (요일 단위로)\n","    def get_kakao_webtoon_info(self):\n","        driver = webdriver.Chrome('chromedriver', options=options)\n","        action = ActionChains(driver)\n","        driver.get(self.kw_url)\n","        sleep(3)\n","        # 로그인 해야 들어갈 수 있는 것들 때문에 일단 로그인하기\n","        self.login_on_kakao_page(driver)\n","\n","        # # 완결 웹툰은 일단 제외하고 요일별 페이지 가져오기\n","        # days = driver.find_elements(By.CLASS_NAME, \"e1201h8a0\")[:-1]\n","        # 완결 웹툰 포함해서 요일별 페이지 가져오기\n","        days = driver.find_elements(By.CLASS_NAME, \"e1201h8a0\")\n","\n","        day_tds = []\n","        filenames = []\n","\n","        idx = 0\n","        for i in range(len(days)):\n","            filename = \"kakao\" + str(i) + \".csv\"\n","            filenames.append(filename)\n","            if os.path.isfile(filename):\n","                day_tds.append(ut.get_from_csv(filename))\n","                idx += len(day_tds[-1])\n","                continue\n","\n","            day_wd = WebtoonData()\n","            total_titles = []\n","\n","            # 요일별 페이지에 있는 웹툰들 가져오기 (스크롤해야 보이는 것 까지 포함)\n","            day = driver.find_elements(By.CLASS_NAME, \"e1201h8a0\")[i]\n","            action.move_to_element(day).click().perform()\n","            if i == 7:\n","                self.do_scroll_down(80, driver)\n","            else:\n","                self.do_scroll_down(10, driver)\n","            webtoons = driver.find_elements(By.CLASS_NAME, \"css-qm6qod\")\n","\n","            # 웹툰별로 정보 저장하기\n","            for j in range(len(webtoons)):\n","                print(\"\\rday[\" + str(i) + \"] - process: \" + str(j + 1) + \" / \" + str(len(webtoons)), end=\"\")\n","                # 해당 웹툰으로 이동하기\n","                webtoon = driver.find_elements(By.CLASS_NAME, \"css-qm6qod\")[j]\n","                action.move_to_element(webtoon).key_down(Keys.CONTROL).click().key_up(Keys.CONTROL).perform()\n","                sleep(2)\n","                driver.switch_to.window(driver.window_handles[1])\n","\n","                # 이미지 정보 먼저 저장하기\n","                html = driver.page_source\n","                soup = bs(html, 'html.parser')\n","                image_url = soup.find('div', {'class': 'css-1y42t5x'}).find('img')\n","                image_url = image_url['src']\n","                image_url = \"https:\" + image_url\n","\n","                # 작품소개 창 열기\n","                notice = driver.find_elements(By.CLASS_NAME, \"jsx-3114325382\")\n","                if notice:\n","                    notice[0].click()\n","                driver.find_element(By.CLASS_NAME, \"css-nxuz68\").click()\n","\n","                # 현재 창에서 데이터 읽기\n","                sleep(0.5)\n","                html = driver.page_source\n","                soup = bs(html, 'html.parser')\n","\n","                title = soup.find('h2', {'class': 'css-jgjrt'}).text\n","                day = soup.find_all('div', {'class': 'css-7a7cma'})[0].text\n","                day_word_end_idx = day.find(\" 연재\")\n","                if day_word_end_idx == -1:\n","                    day = \"완결\"\n","                else:\n","                    day = day[:day_word_end_idx]\n","                author = soup.find_all('div', {'class': 'css-7a7cma'})[1].text\n","                author = author.replace(',', ', ')\n","                genre = soup.find('div', {'class': 'infoBox'})\n","                genre = genre.find_all('div', {'class': 'jsx-3755015728'})[2].text\n","                genre = genre[genre.find(\"웹툰\") + 2:]\n","                story = soup.find('div', {'class': 'descriptionBox'}).text\n","\n","                # 다른 요일에서 이미 추가된거면 스킵하기\n","                if title in total_titles:\n","                    continue\n","\n","                # 리스트에 추가\n","                day_wd.id_list.append(idx)\n","                day_wd.thumbnail_list.append(image_url)\n","                day_wd.title_list.append(title)\n","                day_wd.author_list.append(author)\n","                day_wd.day_list.append(day)\n","                if genre == \"액션무협\":\n","                    day_wd.genre_list.append(\"무협\")\n","                else:\n","                    day_wd.genre_list.append(genre)\n","                day_wd.story_list.append(story)\n","                day_wd.platform_list.append(\"카카오\")\n","                day_wd.url_list.append(driver.current_url)\n","                total_titles.append(title)\n","\n","                idx += 1\n","                # 다시 메인페이지로 돌아가기\n","                driver.close()\n","                driver.switch_to.window(driver.window_handles[0])\n","                sleep(0.5)\n","\n","            day_tds.append(td.make_total_data(day_wd))\n","            ut.make_csv(filename, day_tds[-1])\n","\n","        # 요일별로 만든 dataframe 모두 합치고 기존 것들은 지우기\n","        total_td = pd.concat(day_tds)\n","        total_td = total_td.drop_duplicates(['title'])\n","        total_td['id'] = [i for i in range(len(total_td))]\n","        total_td.set_index('id', inplace=True)\n","\n","        # for filename in filenames:\n","        #     ut.delete_csv(filename)\n","\n","        print()\n","        return total_td\n","\n","    # 카카오 페이지에 로그인 하기\n","    def login_on_kakao_page(self, driver):\n","        # 로그인 페이지로 이동하기\n","        driver.find_elements(By.CLASS_NAME, 'css-vurnku')[1].click()\n","        sleep(1)\n","        driver.switch_to.window(driver.window_handles[1])\n","        # 아이디 및 패스워드 입력하고 로그인 누르기\n","        driver.find_element(By.NAME, 'email').send_keys(self.login_id)\n","        driver.find_element(By.NAME, 'password').send_keys(self.login_pw)\n","        driver.find_element(By.CLASS_NAME, 'btn_confirm').click()\n","        sleep(1)\n","        driver.switch_to.window(driver.window_handles[0])\n","        sleep(1)\n","\n","    # 스크롤을 내려야 나오는 데이터를 얻기 위해 스크롤하기\n","    def do_scroll_down(self, seconds, driver):\n","        start = datetime.datetime.now()\n","        end = start + datetime.timedelta(seconds=seconds)\n","        while True:\n","            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)\n","            sleep(0.5)\n","            if datetime.datetime.now() > end:\n","                break\n"]},{"cell_type":"markdown","source":["# myTokenize"],"metadata":{"id":"_bHKw8_sKhLZ"}},{"cell_type":"code","source":["from konlpy.tag import Kkma\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","class MyTokenize:\n","    pos = (\"N\", \"V\", \"M\", \"XR\")  # 토큰화할 단어 태그 http://kkma.snu.ac.kr/documents/?doc=postag\n","\n","    def __init__(self, data):\n","        self.tfidf_vectorizer = TfidfVectorizer(\n","            tokenizer=self.tokenizer,  # 문장에 대한 tokenizer (위에 정의한 함수 이용)\n","            min_df=1,  # 단어가 출현하는 최소 문서의 개수\n","            sublinear_tf=True  # tf값에 1+log(tf)를 적용하여 tf값이 무한정 커지는 것을 막음\n","        )\n","        self.story_data = data['story']\n","\n","    # 토큰화하기\n","    def tokenizer(self, raw_texts):\n","        kkma = Kkma()\n","        p = kkma.pos(raw_texts)\n","        o = [word for word, tag in p if (len(word) > 1) and (tag.startswith(self.pos))]\n","        return o\n","\n","    # 토큰화 결과 확인해보기\n","    def print_tokenize_result(self):\n","        tokenized_sentence = []\n","        for i, story in enumerate(self.story_data):\n","            tokenized_sentence.append(self.tokenizer(story))\n","            print(str(i+1) + \" / \" + str(len(self.story_data)))\n","            print(story)\n","            print(tokenized_sentence[-1])\n","            print(\"------------------------------------------------------------\")\n","\n","    # 토근화를 기반으로 벡터화하고 데이터 반환하기\n","    def get_vectorized_data(self):\n","        vectorized = self.tfidf_vectorizer.fit_transform(self.story_data)\n","        return vectorized\n"],"metadata":{"id":"tQxwIP-FLXu2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# myStoryClustering"],"metadata":{"id":"K6C-2d7AKnoB"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import font_manager, rc\n","from sklearn.decomposition import TruncatedSVD\n","import seaborn as sns\n","\n","\n","class MyStoryClustering:\n","    top_n_features = 5\n","\n","    def __init__(self, vectorized, vectorizer, total_data):\n","        self.vectorized = vectorized\n","        self.vectorizer = vectorizer\n","        self.data = total_data\n","        self.kmeans = KMeans()\n","\n","        # 한글 폰트 설정\n","        font_path = \"C:/Windows/Fonts/batang.ttc\"\n","        font = font_manager.FontProperties(fname=font_path).get_name()\n","        rc('font', family=font, size=15)\n","\n","    # silhoutte 방법으로 적정 k값 구하기\n","    def get_proper_k(self, data_index):\n","        max_k = len(data_index) // 10\n","\n","        # 데이터 개수가 너무 적으면 k=1로 하기\n","        if max_k <= 1:\n","            proper_k = 1\n","        else:\n","            silhoutte_values = []\n","            for i in range(2, max_k+1):\n","                kmeans = KMeans(n_clusters=i, init='k-means++')\n","                pred = kmeans.fit_predict(self.vectorized[data_index])\n","                silhoutte_values.append(np.mean(silhouette_samples(self.vectorized[data_index], pred)))\n","\n","            proper_k = np.argmax(silhoutte_values) + 2\n","\n","        return proper_k\n","\n","    # K-means로 군집화시키기\n","    def kmeans_cluster(self, genre, data_index, k=-1):\n","        if k == -1:\n","            cluster_num = self.get_proper_k(data_index)\n","        else:\n","            cluster_num = k\n","        print(genre + \": \" + str(cluster_num))\n","        self.kmeans = KMeans(n_clusters=cluster_num)\n","        cluster_label = self.kmeans.fit_predict(self.vectorized[data_index])\n","        return cluster_label\n","\n","    # 군집별 핵심단어 추출하기\n","    def get_cluster_details(self, genre):\n","        # 각 클러스터별 핵심 단어를 저장할 변수\n","        cluster_details = pd.DataFrame({\n","            \"genre\": [],\n","            \"cluster_num\": [],\n","            \"words\": [],\n","        })\n","\n","        feature_names = self.vectorizer.get_feature_names_out()\n","\n","        # 각 클러스터 레이블별 feature들의 center값들 내림차순으로 정렬 후의 인덱스를 반환\n","        center_feature_idx = self.kmeans.cluster_centers_.argsort()[:, ::-1]\n","\n","        # 각 feature별 center값들 정렬한 인덱스 중 상위 값들 추출\n","        top_features = []\n","        for cluster_num in range(len(center_feature_idx)):\n","            top_feature_idx = center_feature_idx[cluster_num, :self.top_n_features]\n","            top_feature = [feature_names[idx] for idx in top_feature_idx]\n","            top_features.append(top_feature)\n","\n","        cluster_details['genre'] = [genre for _ in range(len(center_feature_idx))]\n","        cluster_details['cluster_num'] = range(len(center_feature_idx))\n","        cluster_details['words'] = top_features\n","\n","        return cluster_details\n","\n","    # 유사도 그래프로 비교해보기\n","    def compare_similarity(self, idx, row, cluster):\n","        self.data = self.data.sort_index()\n","\n","        # 해당 행의 웹툰이 어느 클러스터에 속해있고 인덱스는 몇인지 구하기\n","        target_cluster = row[cluster]\n","        target_webtoon_idx = idx\n","        target_genre = row[\"genre\"]\n","\n","        # 해당 클러스트 안에 있는 웹툰들을 모두 구하기\n","        if target_cluster == \"cluster_story_in_genre\":\n","            webtoons_in_target_cluster = self.data[(self.data[cluster] == target_cluster) & (self.data['genre'] == target_genre)]\n","        else:\n","            webtoons_in_target_cluster = self.data[self.data[cluster] == target_cluster]\n","\n","        webtoons_idx = webtoons_in_target_cluster.index\n","\n","        # 위에서 추출한 카테고리로 클러스터링된 문서들의 인덱스 중 비교기준문서를 제외한 다른 문서들과의 유사도 측정\n","        similarity = cosine_similarity(self.vectorized[target_webtoon_idx], self.vectorized[webtoons_idx])\n","\n","        # array 내림차순으로 정렬한 후 인덱스 반환\n","        sorted_idx = np.argsort(similarity)[:, ::-1]\n","        # 비교문서 당사자는 제외한 인덱스 추출 (내림차순 정렬했기때문에 0번째가 무조건 가장 큰 값임)\n","        sorted_idx = sorted_idx[:, 1:]\n","\n","        # index로 넣으려면 1차원으로 reshape해주기\n","        sorted_idx = sorted_idx.reshape(-1)\n","\n","        # 앞에서 구한 인덱스로 유사도 행렬값도 정렬\n","        sorted_sim_values = similarity.reshape(-1)[sorted_idx]\n","\n","        # if idx == 0:\n","        #     print(\"타겟 클러스터 번호:\", target_cluster)\n","        #     print(\"타겟 웹툰 인덱스:\", target_webtoon_idx)\n","        #     print(\"유사도 비교 기준 웹툰:\", row['title'])\n","        #     print(\"유사한 웹툰 인덱스:\")\n","        #     print(list(webtoons_idx))\n","        #     print(\"유사도(내림차순 정렬):\")\n","        #     print(sorted_sim_values)\n","        #     print(len(sorted_sim_values))\n","        #     print(len(webtoons_idx[sorted_idx[:len([x for x in sorted_sim_values if x != 0])]]))\n","        #     print()\n","        #\n","        #     # 그래프 생성\n","        #     selected_sim_df = pd.DataFrame()\n","        #     selected_sim_df['title'] = webtoons_in_target_cluster.iloc[sorted_idx]['title']\n","        #     selected_sim_df['similarity'] = sorted_sim_values\n","        #\n","        #     plt.figure(figsize=(25, 10), dpi=60)\n","        #     sns.barplot(data=selected_sim_df, x='similarity', y='title')\n","        #     plt.title(row['title'])\n","        #     plt.show()\n","\n","        # 높은 유사도 순으로 웹툰 인덱스 반환 (0인 것은 뺌)\n","        return list(webtoons_idx[sorted_idx[:len([x for x in sorted_sim_values if x != 0])]])"],"metadata":{"id":"pEmf7CEALbhj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# myStyleClustering"],"metadata":{"id":"-A-4djgyKsGb"}},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","import tensorflow as tf\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics.pairwise import cosine_similarity\n","import matplotlib.pyplot as plt\n","\n","class MyStyleClustering:\n","    max_dim = 150\n","    min_dim = 100\n","    content_layers = ['block5_conv2']\n","    style_layers = ['block1_conv1',\n","                    'block2_conv1',\n","                    'block3_conv1',\n","                    'block4_conv1',\n","                    'block5_conv1']\n","\n","    def __init__(self, data):\n","        self.data = data\n","        self.style_info_list = np.array([])\n","\n","    # 이미지 크기 바꾸기\n","    def resize_img(self, img_path):\n","        img = tf.io.read_file(img_path)\n","        img = tf.image.decode_image(img, channels=3)\n","        img = tf.image.convert_image_dtype(img, tf.float32)\n","        img = np.squeeze(img)\n","\n","        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n","        long_dim = max(shape)\n","        scale = self.max_dim / long_dim\n","\n","        new_shape = tf.cast(shape * scale, tf.int32)\n","        img = tf.image.resize(img, new_shape)\n","        img = img[tf.newaxis, :]\n","\n","        return img\n","\n","    # url 로부터 이미지 가져오기\n","    def get_img(self):\n","        images = []\n","        thumbnails_size = len(self.data['thumbnail'])\n","        for i in range(thumbnails_size):\n","            print(\"\\r\" + str(i + 1) + \"/\" + str(thumbnails_size), end=\"\")\n","            images.append(self.resize_img((\"data/images/thumbnail\" + str(i) + \".jpg\")))\n","        print()\n","        return images\n","\n","    # img = tf.keras.utils.get_file('thumbnail' + str(i + 1) + '.jpg', thumbnail)\n","\n","    # 학습을 시키면서 스타일 추출해내기\n","    def test_extract_style(self, style_image, train_n):\n","        content_image = np.ones((1, 100, 100, 3))\n","\n","        extractor = StyleContentModel(self.style_layers, self.content_layers)\n","        style_targets = extractor.call(style_image)['style']\n","        content_targets = extractor.call(content_image)['content']\n","\n","        image = tf.Variable(content_image)\n","\n","        opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n","\n","        squeezed_img = np.array(content_image).squeeze()\n","        plt.subplot(2, 3, 1)\n","        plt.imshow(squeezed_img)\n","        plt.title(\"Before\")\n","\n","        squeezed_img = np.array(style_image).squeeze()\n","        plt.subplot(2, 3, 3)\n","        plt.imshow(squeezed_img)\n","        plt.title(\"Before\")\n","\n","        for step in range(train_n):\n","            print(\"\\r\" + str(step + 1) + \"/\" + str(train_n), end=\"\")\n","            self.train_step(image=image,\n","                            style_targets=style_targets,\n","                            content_targets=content_targets,\n","                            opt=opt,\n","                            extractor=extractor)\n","\n","        result_image = np.array(image).squeeze()\n","        plt.subplot(2, 3, 5)\n","        plt.imshow(result_image)\n","        plt.title(\"After\")\n","\n","        plt.show()\n","\n","\n","    # 학습을 시키면서 스타일 추출해내기\n","    def extract_style(self, images, train_n):\n","        trained_images = []\n","        tf_var = tf.Variable(images[0][:, :self.min_dim, :self.min_dim, :])\n","        extractor = StyleContentModel()\n","        opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n","\n","        for i, img in enumerate(images):\n","\n","            print(\"\\r\" + str(i + 1) + \"/\" + str(len(images)), end=\"\")\n","            cropped_img = img[:, :self.min_dim, :self.min_dim, :]\n","\n","            style_targets = extractor.call(cropped_img)\n","            tf_var.assign(cropped_img)\n","\n","            for step in range(train_n):\n","                self.train_step(style_targets=style_targets,\n","                                opt=opt,\n","                                extractor=extractor,\n","                                image=tf_var)\n","\n","            result_image = np.array(tf_var)\n","            result_image = result_image.reshape(-1)\n","            trained_images.append(result_image)\n","\n","        return trained_images\n","\n","    def svd(self, trained_images, svd_n):\n","        print()\n","        if svd_n == 0:\n","            self.style_info_list = trained_images\n","        else:\n","            svd = TruncatedSVD(n_components=svd_n)\n","            self.style_info_list = np.array(svd.fit_transform(trained_images))\n","\n","    # 스타일 학습시킬 때 쓸 loss 함수\n","    def style_content_loss(self, outputs, style_targets, content_targets):\n","        style_weight = 1e4\n","        content_weight = 1e-2\n","\n","        style_outputs = outputs['style']\n","        content_outputs = outputs['content']\n","        style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2)\n","                               for name in style_outputs.keys()])\n","        style_loss *= style_weight / len(self.style_layers)\n","\n","        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2)\n","                                 for name in content_outputs.keys()])\n","        content_loss *= content_weight / len(self.content_layers)\n","        loss = style_loss + content_loss\n","        return loss\n","\n","    # 픽셀 값이 실수이므로 0과 1 사이의 값으로 바꾸기\n","    def clip_0_1(self, image):\n","        return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n","\n","    # 학습하기\n","    @tf.function()\n","    def train_step(self, image, style_targets, content_targets, opt, extractor):\n","        with tf.GradientTape() as tape:\n","            outputs = extractor(image)\n","            loss = self.style_content_loss(outputs=outputs,\n","                                           style_targets=style_targets,\n","                                           content_targets=content_targets)\n","\n","\n","        grad = tape.gradient(loss, image)\n","        opt.apply_gradients([(grad, image)])\n","        image.assign(self.clip_0_1(image))\n","\n","    # tensor 자료형을 이미지로 변환해주기\n","    def tensor_to_image(self, tensor):\n","        tensor = tensor * 255\n","        tensor = np.array(tensor, dtype=np.uint8)\n","        if np.ndim(tensor) > 3:\n","            assert tensor.shape[0] == 1\n","            tensor = tensor[0]\n","        return PIL.Image.fromarray(tensor)\n","\n","    # 이미지 plt로 보여주기\n","    def imshow(self, image, title=None):\n","        # self.tensor_to_image(result_image)\n","        # plt.subplot(1, 2, 2)\n","        # self.imshow(result_image, \"After\")\n","        # plt.show()\n","        if len(image.shape) > 3:\n","            image = tf.squeeze(image, axis=0)\n","\n","        plt.imshow(image)\n","        if title:\n","            plt.title(title)\n","\n","    # k-means 클러스터링 하기\n","    def kmeans_cluster(self, k):\n","        kmeans = KMeans(n_clusters=k)\n","        pred = kmeans.fit_predict(self.style_info_list)\n","        return pred\n","\n","    # 클러스터링 결과 그래프로 보여주기\n","    def visualize(self, cluster_labels):\n","        fig = plt.figure(figsize=(6,4))\n","        colors = plt.cm.get_cmap(\"Spectral\")(np.linspace(0, 1, len(set(cluster_labels))))\n","        ax = fig.add_subplot(1, 1, 1)\n","\n","        for k, col in zip(range(len(colors)), colors):\n","            my_members = (cluster_labels == k)\n","            ax.plot(\n","                self.style_info_list[my_members, 0],\n","                self.style_info_list[my_members, 1],\n","                'w',\n","                markerfacecolor=col,\n","                marker='.'\n","            )\n","        ax.set_title('K-Means')\n","\n","        plt.show()\n","\n","    # 유사도 그래프로 비교해보기\n","    def compare_similarity(self, idx, row):\n","        self.data = self.data.sort_index()\n","\n","        # 해당 행의 웹툰이 어느 클러스터에 속해있고 인덱스는 몇인지 구하기\n","        target_cluster = row[\"cluster_style\"]\n","        target_webtoon_idx = idx\n","\n","        # 해당 클러스트 안에 있는 웹툰들을 모두 구하기\n","        webtoons_in_target_cluster = self.data[self.data[\"cluster_style\"] == target_cluster]\n","\n","        webtoons_idx = webtoons_in_target_cluster.index\n","\n","        # 위에서 추출한 카테고리로 클러스터링된 문서들의 인덱스 중 비교기준문서를 제외한 다른 문서들과의 유사도 측정\n","        similarity = cosine_similarity(self.style_info_list[target_webtoon_idx].reshape(1, -1), self.style_info_list[webtoons_idx])\n","\n","        # array 내림차순으로 정렬한 후 인덱스 반환\n","        sorted_idx = np.argsort(similarity)[:, ::-1]\n","        # 비교문서 당사자는 제외한 인덱스 추출 (내림차순 정렬했기때문에 0번째가 무조건 가장 큰 값임)\n","        sorted_idx = sorted_idx[:, 1:]\n","\n","        # index로 넣으려면 1차원으로 reshape해주기\n","        sorted_idx = sorted_idx.reshape(-1)\n","\n","        # 앞에서 구한 인덱스로 유사도 행렬값도 정렬\n","        sorted_sim_values = similarity.reshape(-1)[sorted_idx]\n","\n","        # 높은 유사도 순으로 웹툰 인덱스 반환 (0인 것은 뺌)\n","        return list(webtoons_idx[sorted_idx[:len([x for x in sorted_sim_values if x != 0])]])\n","\n","# 스타일 추출하는 모델 정의하기\n","class StyleContentModel(tf.keras.models.Model):\n","    def __init__(self, style_layers, content_layers):\n","        super(StyleContentModel, self).__init__()\n","        self.vgg = self.vgg_layers(style_layers + content_layers)\n","        self.style_layers = style_layers\n","        self.content_layers = content_layers\n","        self.num_style_layers = len(style_layers)\n","        self.vgg.trainable = False\n","\n","    # vgg 모델 불러오기\n","    def vgg_layers(self, layer_names):\n","        # 이미지넷 데이터셋에 사전학습된 VGG 모델 불러오기\n","        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","        vgg.trainable = False\n","\n","        outputs = [vgg.get_layer(name).output for name in layer_names]\n","\n","        model = tf.keras.Model([vgg.input], outputs)\n","        return model\n","\n","    # 스타일을 뽑아내기 위한 그람 행렬\n","    def gram_matrix(self, input_tensor):\n","        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n","        input_shape = tf.shape(input_tensor)\n","        num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n","        return result / num_locations\n","\n","    def call(self, inputs):\n","        # \"[0,1] 사이의 실수 값을 입력으로 받습니다\"\n","        inputs = inputs * 255.0\n","        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","        outputs = self.vgg(preprocessed_input)\n","        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n","                                          outputs[self.num_style_layers:])\n","\n","        style_outputs = [self.gram_matrix(style_output)\n","                         for style_output in style_outputs]\n","\n","        content_dict = {content_name: value\n","                        for content_name, value\n","                        in zip(self.content_layers, content_outputs)}\n","\n","        style_dict = {style_name: value\n","                      for style_name, value\n","                      in zip(self.style_layers, style_outputs)}\n","\n","        return {'content': content_dict, 'style': style_dict}"],"metadata":{"id":"WYZrx5poLi_s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"CJ4PyYGeLoZI"}},{"cell_type":"code","source":["import os.path\n","\n","import pandas as pd\n","\n","from totalData import TotalData as td\n","from myWebCrawling import MyWebCrawling\n","from myUtil import MyUtil as ut\n","from myTokenize import MyTokenize\n","from myStoryClustering import MyStoryClustering\n","from myStyleClustering import MyStyleClustering\n","\n","\n","naver_csv_filename = \"data/네이버웹툰정보.csv\"\n","kakao_csv_filename = \"data/카카오웹툰정보.csv\"\n","cluster_csv_filename = \"data/클러스터정보.csv\"\n","cluster_detail_csv_filename = \"data/클러스터상위단어.csv\"\n","vector_filename = \"data/vector_data.pickle\"\n","images_filename = \"data/images.pickle\"\n","trained_images_filename = \"data/trained_images.pickle\"\n","\n","# 네이버 및 카카오 웹툰 크롤링 하기 (새로 하기 또는 저장된 데이터 불러오기)\n","def do_web_crawling():\n","    wc = MyWebCrawling()\n","\n","    # 네이버 웹툰 정보 가져오기\n","    print(\"--naver webtoon crawling start--\")\n","    if not os.path.isfile(naver_csv_filename):\n","        naver_td = wc.get_naver_webtoon_info()\n","        ut.make_csv(naver_csv_filename, naver_td)\n","    else:\n","        naver_td = ut.get_from_csv(naver_csv_filename)\n","    print(\"--naver webtoon crawling end--\")\n","\n","    # 카카오 웹툰 정보 가져오기\n","    print(\"--kakao webtoon crawling start--\")\n","    if not os.path.isfile(kakao_csv_filename):\n","        # 데이터 양이 많아서 요일별로 끊어서 파일 만들고 합치기\n","        kakao_td = wc.get_kakao_webtoon_info()\n","        ut.make_csv(kakao_csv_filename, kakao_td)\n","    else:\n","        kakao_td = ut.get_from_csv(kakao_csv_filename)\n","    print(\"--kakao webtoon crawling end--\")\n","\n","    # 가져온 데이터들 합치기\n","    td.merge_total_data([naver_td, kakao_td])\n","    # td.total_data = naver_td\n","    # td.total_data = kakao_td\n","    # 웹툰 카테고리 분류하기\n","    td.save_category()\n","\n","# 토큰화 및 벡터화하기 (새로 하기 또는 저장된 데이터 불러오기)\n","def do_tokenize_and_vectorize():\n","    print(\"--vectorized start--\")\n","    if not os.path.isfile(vector_filename):\n","        tk = MyTokenize(td.total_data)\n","        vectorized = tk.get_vectorized_data()\n","        vectorizer = tk.tfidf_vectorizer\n","        ut.save_data(vector_filename, (vectorized, vectorizer))\n","    else:\n","        vectorized, vectorizer = ut.load_data(vector_filename)\n","    print(\"--vectorized end--\")\n","    story_ct = MyStoryClustering(vectorized, vectorizer, td.total_data)\n","    return story_ct\n","\n","# story에 대한 k-means 클러스터링 하기\n","def do_clustering_by_story(story_ct, k_for_total=175):\n","    print(\"--kmeans story clustering start--\")\n","    print(\"\\n<적정 k값>\")\n","\n","    cluster_details_list = []\n","\n","    # 전체 웹툰 안에서 클러스터링 하기\n","    total_index = list(range(len(td.total_data)))\n","    cluster_labels_for_whole = story_ct.kmeans_cluster(\"전체\", total_index, k=k_for_total)\n","    cluster_details = story_ct.get_cluster_details(\"전체\")\n","    cluster_details_list.append(cluster_details)\n","    # story_ct.visualize(cluster_labels_for_whole)\n","\n","    # 각 장르 안에서 클러스터링 하기\n","    cluster_labels_for_genre = [-1 for _ in range(len(td.total_data))]\n","    for genre in td.categories:\n","        current_data_index = td.total_data.index[td.total_data['genre'] == genre].tolist()\n","        current_data_index = list(map(int, current_data_index))\n","        cluster_label = story_ct.kmeans_cluster(genre, current_data_index)\n","        for i in range(len(current_data_index)):\n","            cluster_labels_for_genre[current_data_index[i]] = cluster_label[i]\n","        cluster_details = story_ct.get_cluster_details(genre)\n","        cluster_details_list.append(cluster_details)\n","\n","    print()\n","    td.total_data[\"cluster_story\"] = cluster_labels_for_whole\n","    td.total_data[\"cluster_story_in_genre\"] = cluster_labels_for_genre\n","    td.cluster_details = pd.concat(cluster_details_list)\n","    print(\"--kmeans story clustering end--\")\n","\n","# style에 대한 k-means 클러스터링 하기\n","def do_clustering_by_style(style_ct, k, train_n, svd_n):\n","    # 이미지 로딩하기 (새로 하기 또는 저장된 데이터 불러오기)\n","    print(\"--images loading start--\")\n","    thumbnails = style_ct.get_img()\n","    print(\"--images loading end--\")\n","    print(\"--style extraction start--\")\n","    # 각 이미지마다 스타일 추출하기\n","    index = 238\n","    # index = 1295\n","    print(td.total_data['title'][index])\n","    style_ct.test_extract_style(thumbnails[index], 100)\n","    # if not os.path.isfile(trained_images_filename):\n","    #     trained_images = style_ct.extract_style(thumbnails, train_n)\n","    #     ut.save_data(trained_images_filename, trained_images)\n","    # else:\n","    #     trained_images = ut.load_data(trained_images_filename)\n","    # style_ct.svd(trained_images, svd_n)\n","    # print(\"--style extraction end--\")\n","    # print(\"--kmeans style clustering start--\")\n","    # # 추출한 스타일로 k-means 클러스터링 하기\n","    # cluster_labels = style_ct.kmeans_cluster(k)\n","    # td.total_data[\"cluster_style\"] = cluster_labels\n","    # if svd_n == 2:\n","    #     style_ct.visualize(cluster_labels)\n","    # print(\"--kmeans style clustering end--\")\n","\n","# 각 클러스터에 대해 유사도가 높은 데이터만 따로 정리해놓기\n","def arrange_high_similarity_webtoons(story_ct, style_ct):\n","    print(\"-- similarity calculation start--\")\n","    cluster_story_group = []\n","    cluster_story_group_in_genre = []\n","    cluster_style_group = []\n","    for idx, row in td.total_data.iterrows():\n","        cluster_story_group.append(story_ct.compare_similarity(idx, row, \"cluster_story\"))\n","        cluster_story_group_in_genre.append(story_ct.compare_similarity(idx, row, \"cluster_story_in_genre\"))\n","        cluster_style_group.append(style_ct.compare_similarity(idx, row))\n","    td.total_data[\"cluster_story_group\"] = cluster_story_group\n","    td.total_data[\"cluster_story_group_in_genre\"] = cluster_story_group_in_genre\n","    td.total_data[\"cluster_style_group\"] = cluster_style_group\n","    print(\"-- similarity calculation end--\")\n","\n","if __name__ == '__main__':\n","    ut.make_data_folder()\n","    do_web_crawling()\n","    # story_clustering = do_tokenize_and_vectorize()\n","    # do_clustering_by_story(story_clustering, k_for_total=64)\n","    ut.save_images(td.total_data['thumbnail'])\n","    style_clustering = MyStyleClustering(td.total_data)\n","    do_clustering_by_style(style_clustering, k=32, train_n=256, svd_n=2)\n","    # arrange_high_similarity_webtoons(story_clustering, style_clustering)\n","    # ut.make_csv(cluster_csv_filename, td.total_data)\n","    # ut.make_csv(cluster_csv_filename, td.total_data)\n","    # ut.make_csv(cluster_detail_csv_filename, td.cluster_details)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"P7dcmgocLsVQ","executionInfo":{"status":"error","timestamp":1667542079412,"user_tz":-540,"elapsed":123384,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"469b3f83-36a4-43ea-cf36-f788e6571db1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--naver webtoon crawling start--\n","--naver webtoon crawling end--\n","\n","<웹툰 카테고리 종류 및 개수>\n","전체: 1913\n","무협: 40\n","감성: 33\n","개그: 138\n","로맨스: 465\n","스포츠: 23\n","스릴러: 241\n","판타지: 338\n","액션: 140\n","일상: 80\n","드라마: 415\n","\n","--vectorized start--\n","--vectorized end--\n","--kmeans story clustering start--\n","\n","<적정 k값>\n","전체: 64\n","무협: 4\n","감성: 2\n","개그: 10\n","로맨스: 42\n","스포츠: 2\n","스릴러: 24\n","판타지: 32\n","액션: 14\n","일상: 8\n","드라마: 41\n","\n","--kmeans story clustering end--\n","--images already exist--\n","--images loading start--\n","1913/1913\n","--images loading end--\n","--style extraction start--\n"]},{"output_type":"stream","name":"stderr","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-0ef1d31e0457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'thumbnail'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mstyle_clustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyStyleClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdo_clustering_by_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0marrange_high_similarity_webtoons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_clustering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_csv_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-0ef1d31e0457>\u001b[0m in \u001b[0;36mdo_clustering_by_style\u001b[0;34m(style_ct, k, train_n, svd_n)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# 각 이미지마다 스타일 추출하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_images_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtrained_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_ct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_extract_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthumbnails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_images_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-88cb124404d8>\u001b[0m in \u001b[0;36mtest_extract_style\u001b[0;34m(self, style_image, train_n)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleContentModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mstyle_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mcontent_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-88cb124404d8>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# \"[0,1] 사이의 실수 값을 입력으로 받습니다\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mpreprocessed_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"]}]},{"cell_type":"markdown","metadata":{"id":"LA9FGDP7V6tZ"},"source":["# 깃허브 관련"]},{"cell_type":"markdown","metadata":{"id":"2--jwgVwZb9Z"},"source":["취소 관련   \n","https://gmlwjd9405.github.io/2018/05/25/git-add-cancle.html"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"NVDIHfH7OcYE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667833540722,"user_tz":-540,"elapsed":346,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"9d0f109b-998d-42ea-a805-80398c84add6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Github/IndividualProject-webtoonRecommendation\n"]}],"source":["cd drive/MyDrive/Colab\\ Notebooks/Github/IndividualProject-webtoonRecommendation"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"xKBdElx8VoFc","executionInfo":{"status":"ok","timestamp":1667833572969,"user_tz":-540,"elapsed":349,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"outputs":[],"source":["!git config --global user.email \"lightcloud98@gmail.com\"\n","!git config --global user.name \"ddophi98\""]},{"cell_type":"code","execution_count":15,"metadata":{"id":"9fFNGHLQVE5n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667833577043,"user_tz":-540,"elapsed":281,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"462c6867-aff0-4daf-8e1f-91cbc9444cb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any parent up to mount point /content)\n","Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"]}],"source":["!git add WentoonRecommendML.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L_-LAUZX_Cm"},"outputs":[],"source":["!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6GkdJmjV_PD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667369794841,"user_tz":-540,"elapsed":7500,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"8f0d9ad7-a94e-4c2d-ce1b-0fcf54373b36"},"outputs":[{"output_type":"stream","name":"stdout","text":["[main e3b1032] 테스트 이미지 실험\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite LoFi_To_HiFi.ipynb (96%)\n"]}],"source":["!git commit -m \"테스트 이미지 실험\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa5WplLwYOh-"},"outputs":[],"source":["!git log"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K33gtod9VVZk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667369801698,"user_tz":-540,"elapsed":6459,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"8b0a99f0-ad6a-416b-df7d-6baa3ae255ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Counting objects: 1   \rCounting objects: 3, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:  33% (1/3)   \rCompressing objects:  66% (2/3)   \rCompressing objects: 100% (3/3)   \rCompressing objects: 100% (3/3), done.\n","Writing objects:  33% (1/3)   \rWriting objects:  66% (2/3)   \rWriting objects: 100% (3/3)   \rWriting objects: 100% (3/3), 70.19 KiB | 3.34 MiB/s, done.\n","Total 3 (delta 1), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To https://github.com/ddophi98/IndividualProjcet-lofiToHifi.git\n","   c6c1c3e..e3b1032  main -> main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n"]}],"source":["!git push -u origin main"]}]}