{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["QxE72Sk2OyoU","EYpGyNd1Rzs5","Gb88r205K0AE","MdEW3OqxK48_","G5HF7pziK-BR","Zs8-qTRcKRE6","_bHKw8_sKhLZ","K6C-2d7AKnoB","-A-4djgyKsGb","crCDuuT97PPV","ovNS0GVM5x3d"],"authorship_tag":"ABX9TyMq+fUusr0Q6raSn+nrjnYI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0FHWcwIktk0","executionInfo":{"status":"ok","timestamp":1669463888308,"user_tz":-540,"elapsed":6833,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"80f1bc4f-036d-4c85-a1eb-3a861f4e7549"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Gloabal Variable"],"metadata":{"id":"QxE72Sk2OyoU"}},{"cell_type":"code","source":["data_path = \"/content/drive/MyDrive/Colab Notebooks/Github/IndividualProject-webtoonRecommendation/DeveloperAcademy-NC2-WebtoonRecommendation/MachineLearning/data/\"\n","private_info_path = \"/content/drive/MyDrive/Colab Notebooks/Github/IndividualProject-webtoonRecommendation/DeveloperAcademy-NC2-WebtoonRecommendation/MachineLearning/privateinfo.txt\"\n","nw_url = 'https://comic.naver.com/webtoon/'\n","kw_url = 'https://page.kakao.com/main?categoryUid=10&subCategoryUid=1002'\n","login_id = ''\n","login_pw = ''\n","\n","with open(private_info_path, \"r\") as f:\n","    login_id = f.readline()\n","    login_pw = f.readline()"],"metadata":{"id":"H3OVzByPO04z","executionInfo":{"status":"ok","timestamp":1669463888308,"user_tz":-540,"elapsed":4,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# download library"],"metadata":{"id":"EYpGyNd1Rzs5"}},{"cell_type":"code","source":["!pip install selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","\n","!pip install konlpy\n","\n","!pip install mahotas\n","\n","!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"],"metadata":{"id":"YZUjKbJaR2q3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669463918482,"user_tz":-540,"elapsed":30178,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"c33340e9-9c7d-46bb-b757-97c9cd5c7534"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.7/dist-packages (from selenium) (2022.9.24)\n","Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.22.0)\n","Requirement already satisfied: urllib3[socks]~=1.26 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.26.13)\n","Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.9.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.3.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n","Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.10)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.0.4)\n","Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n","Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Hit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n","Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Fetched 261 kB in 2s (110 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","chromium-chromedriver is already the newest version (107.0.5304.87-0ubuntu11.18.04.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n","cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.13)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.21.6)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","fonts-nanum is already the newest version (20170925-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'sudo apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n","/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n","/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n","/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n","/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n","/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n","/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n","/root/.local/share/fonts: skipping, no such directory\n","/root/.fonts: skipping, no such directory\n","/var/cache/fontconfig: cleaning cache directory\n","/root/.cache/fontconfig: not cleaning non-existent cache directory\n","/root/.fontconfig: not cleaning non-existent cache directory\n","fc-cache: succeeded\n"]}]},{"cell_type":"markdown","source":["# myUtil"],"metadata":{"id":"Gb88r205K0AE"}},{"cell_type":"code","source":["import pandas as pd\n","import pickle as pk\n","from urllib import request\n","from PIL import Image\n","import os\n","\n","\n","class ut():\n","    # 모아둔 정보로 CSV 파일 생성하기\n","    @staticmethod\n","    def make_csv(filename, ti):\n","        ti.to_csv(filename, encoding='utf-8-sig')\n","\n","    # CSV 파일이 있다면 가져오기\n","    @staticmethod\n","    def get_from_csv(filename):\n","        return pd.read_csv(filename, encoding='utf-8-sig')\n","\n","    # CSV 파일을 지우기\n","    @staticmethod\n","    def delete_csv(filename):\n","        if os.path.isfile(filename):\n","            os.path.isdir\n","            os.remove(filename)\n","\n","    # 특정 데이터를 파일에 저장해놓기\n","    @staticmethod\n","    def save_data(filename, data):\n","        with open(filename, 'wb') as f:\n","            pk.dump(data, f)\n","\n","    # 저장해둔 데이터 불러오기\n","    @staticmethod\n","    def load_data(filename):\n","        with open(filename, 'rb') as f:\n","            return pk.load(f)\n","\n","    # 이미지 다운받아서 저장하기\n","    @staticmethod\n","    def save_images(urls):\n","        if not os.path.isdir(data_path+\"images\"):\n","            os.makedirs(data_path+\"images\")\n","            os.makedirs(data_path+\"resized_images\")\n","            print(\"--images downloading start--\")\n","            for idx, url in enumerate(urls):\n","                print(\"\\r\" + str(idx + 1) + \"/\" + str(len(urls)), end=\"\")\n","                img_name = data_path+\"images/thumbnail\" + str(idx) + \".jpg\"\n","                request.urlretrieve(url, img_name)\n","                resized_img_name = data_path+\"resized_images/thumbnail\" + str(idx) + \".jpg\"\n","                img = Image.open(img_name).convert('RGB')\n","                img.save(resized_img_name, 'JPEG', qualty=85)\n","\n","            print()\n","            print(\"--images downloading end--\")\n","        else:\n","            print(\"--images already exist--\")\n","\n","    # 이미지에서 추출한 스타일 이미지 저장하기\n","    @staticmethod\n","    def save_style_images(imgs):\n","        if not os.path.isdir(data_path+\"style_images\"):\n","            os.makedirs(data_path+\"style_images\")\n","            for idx, img in enumerate(imgs):\n","                img = Image.fromarray((img * 255).astype(np.uint8))\n","                img_name = data_path+\"style_images/thumbnail\" + str(idx) + \".jpg\"\n","                img.save(img_name, 'JPEG')\n","\n","     # 이미지 크기 바꾸기\n","    @staticmethod\n","    def resize_img(img_path):\n","        max_dim = 150\n","\n","        img = tf.io.read_file(img_path)\n","        img = tf.image.decode_image(img, channels=3)\n","        img = tf.image.convert_image_dtype(img, tf.float32)\n","        img = np.squeeze(img)\n","\n","        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n","        long_dim = max(shape)\n","        scale = max_dim / long_dim\n","\n","        new_shape = tf.cast(shape * scale, tf.int32)\n","        img = tf.image.resize(img, new_shape)\n","        img = img[tf.newaxis, :]\n","\n","        return img\n","\n","    # 폴더로부터 이미지 가져오기\n","    @staticmethod\n","    def get_imges():\n","        thumbnails_size = len(td.total_data['thumbnail'])\n","        images = []\n","        print(\"--images loading from folder start--\")\n","        for i in range(thumbnails_size):\n","            print(\"\\r\" + str(i + 1) + \"/\" + str(thumbnails_size), end=\"\")\n","            images.append(ut.resize_img((data_path+\"images/thumbnail\" + str(i) + \".jpg\")))\n","        print()\n","        print(\"--images loading from folder end--\")\n","        return images\n","        \n"],"metadata":{"id":"YOU4KgmzLJxr","executionInfo":{"status":"ok","timestamp":1669463918483,"user_tz":-540,"elapsed":6,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# totalData"],"metadata":{"id":"MdEW3OqxK48_"}},{"cell_type":"code","source":["import pandas as pd\n","\n","\n","class td:\n","    # 전체 정보를 저장할 변수\n","    total_data = pd.DataFrame({\n","        \"id\": [],\n","        \"cluster_story\": [],\n","        \"cluster_story_in_genre\": [],\n","        \"cluster_style\": [],\n","        \"cluster_story_group\": [],\n","        \"cluster_story_group_in_genre\": [],\n","        \"cluster_style_group\": [],\n","        \"thumbnail\": [],\n","        \"title\": [],\n","        \"author\": [],\n","        \"day\": [],\n","        \"genre\": [],\n","        \"story\": [],\n","        \"platform\": [],\n","        \"url\": [],\n","    })\n","\n","    # 각 클러스터별 핵심 단어를 저장할 변수\n","    cluster_details = pd.DataFrame({\n","        \"genre\": [],\n","        \"cluster_num\": [],\n","        \"words\": [],\n","    })\n","\n","    # 카테고리 목록\n","    categories = []\n","\n","    @staticmethod\n","    def make_total_data(wd):\n","        my_total_data = pd.DataFrame({\n","            \"id\": [],\n","            \"thumbnail\": [],\n","            \"title\": [],\n","            \"author\": [],\n","            \"day\": [],\n","            \"genre\": [],\n","            \"story\": [],\n","            \"platform\": [],\n","            \"url\": [],\n","        })\n","\n","        my_total_data['id'] = wd.id_list\n","        my_total_data['thumbnail'] = wd.thumbnail_list\n","        my_total_data['title'] = wd.title_list\n","        my_total_data['author'] = wd.author_list\n","        my_total_data['day'] = wd.day_list\n","        my_total_data['genre'] = wd.genre_list\n","        my_total_data['story'] = wd.story_list\n","        my_total_data['platform'] = wd.platform_list\n","        my_total_data['url'] = wd.url_list\n","\n","        my_total_data = my_total_data.drop_duplicates(['title'])\n","        my_total_data.set_index('id', inplace=True)\n","\n","        return my_total_data\n","\n","    @staticmethod\n","    def merge_total_data(tds):\n","        td.total_data = pd.concat(tds)\n","        first_td_len = len(tds[0])\n","        td.total_data['id'] = [i for i in range(len(td.total_data))]\n","        td.total_data.set_index('id', inplace=True)\n","        td.total_data = td.total_data.loc[:, ~td.total_data.columns.str.contains('^Unnamed')]\n","\n","    @staticmethod\n","    def save_category():\n","        td.categories = list(set(td.total_data['genre']))\n","        print(\"\\n<웹툰 카테고리 종류 및 개수>\")\n","        print(\"전체: \" + str(len(td.total_data)))\n","        for genre in td.categories:\n","            print(genre + \": \" + str(len(td.total_data.index[td.total_data['genre'] == genre])))\n","        print()\n","\n","    @staticmethod\n","    def title_to_index(title):\n","        return td.total_data.index[td.total_data['title'] == title][0]\n","\n","    @staticmethod\n","    def index_to_title(index):\n","        return td.total_data['title'][index]\n"],"metadata":{"id":"xn7QTjb2LNSt","executionInfo":{"status":"ok","timestamp":1669463918483,"user_tz":-540,"elapsed":5,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# webtoonData"],"metadata":{"id":"G5HF7pziK-BR"}},{"cell_type":"code","source":["class WebtoonData:\n","    def __init__(self):\n","        self.id_list = []\n","        self.thumbnail_list = []\n","        self.title_list = []\n","        self.author_list = []\n","        self.day_list = []\n","        self.genre_list = []\n","        self.story_list = []\n","        self.platform_list = []\n","        self.url_list = []"],"metadata":{"id":"QVzBmmToLRlx","executionInfo":{"status":"ok","timestamp":1669463918483,"user_tz":-540,"elapsed":5,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# myWebCrawling"],"metadata":{"id":"Zs8-qTRcKRE6"}},{"cell_type":"code","source":["from selenium import webdriver\n"," \n","#Colab에선 웹브라우저 창이 뜨지 않으므로 별도 설정한다.\n"," \n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless')        # Head-less 설정\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')"],"metadata":{"id":"xT25OUPlN-w3","executionInfo":{"status":"ok","timestamp":1669463918484,"user_tz":-540,"elapsed":6,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{"id":"yGiCtPa3IXcA","executionInfo":{"status":"ok","timestamp":1669463919157,"user_tz":-540,"elapsed":678,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"outputs":[],"source":["import pandas as pd\n","import requests\n","import datetime\n","from bs4 import BeautifulSoup as bs\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.common.keys import Keys\n","from selenium.webdriver import ActionChains\n","from time import sleep\n","import os.path\n","\n","\n","class MyWebCrawling:\n","    def get_weekday_info(self):\n","        driver = webdriver.Chrome('chromedriver', options=options)\n","        html = requests.get(self.nw_url+\"weekday\").text\n","        soup = bs(html, 'html.parser')\n","        title = soup.find_all('a', {'class': 'title'})\n","        driver.get(self.nw_url+\"weekday\")\n","\n","        part_wd = WebtoonData()\n","        # 각각의 웹툰 정보 수집 시작\n","        idx = 0\n","        for i in range(len(title)):\n","            sleep(0.5)\n","            print(\"\\rprocess(weekday)): \" + str(i + 1) + \" / \" + str(len(title)), end=\"\")\n","            # 월요일 첫 번째 웹툰부터 순서대로 클릭\n","            page = driver.find_elements(By.CLASS_NAME, \"title\")\n","            page[i].click()\n","\n","            # 이동한 페이지 주소 읽고 파싱\n","            html = driver.page_source\n","            soup = bs(html, 'html.parser')\n","\n","            # 요일 수집\n","            day = soup.find_all('ul', {'class': 'category_tab'})\n","            day = day[0].find('li', {'class': 'on'}).text[0:1]\n","\n","            # 요일 두 개 이상이면 요일만 추가함\n","            current_title = title[i].text\n","            if current_title in part_wd.title_list:\n","                part_wd.day_list[part_wd.title_list.index(current_title)] += ', ' + day\n","                driver.back()\n","                continue\n","\n","            # 나머지 정보 수집\n","            image_url = soup.find('div', {'class': 'thumb'}).find('a').find('img')\n","            image_url = image_url['src']\n","            author = soup.find('span', {'class': 'wrt_nm'}).text[8:]\n","            author = author.replace(' / ', ', ')\n","            genre = soup.find('span', {'class': 'genre'}).text.split(\", \")\n","            story = soup.find('div', {'class': 'detail'}).find('p').text\n","\n","            # 리스트에 추가\n","            part_wd.id_list.append(idx)\n","            part_wd.thumbnail_list.append(image_url)\n","            part_wd.title_list.append(current_title)\n","            part_wd.author_list.append(author)\n","            part_wd.day_list.append(day)\n","            if genre[1] == \"무협/사극\":\n","                part_wd.genre_list.append(\"무협\")\n","            else:\n","                part_wd.genre_list.append(genre[1])\n","            part_wd.story_list.append(story)\n","            part_wd.platform_list.append(\"네이버\")\n","            part_wd.url_list.append(driver.current_url)\n","\n","            # 뒤로 가기\n","            idx += 1\n","            driver.back()\n","            sleep(0.5)\n","        return part_wd\n","\n","    def get_finish_info(self):\n","        driver = webdriver.Chrome('chromedriver', options=options)\n","        html = requests.get(self.nw_url + \"finish\").text\n","        soup = bs(html, 'html.parser')\n","        thumb = soup.find_all('div', {'class': 'thumb'})\n","        driver.get(self.nw_url + \"finish\")\n","\n","        part_wd = WebtoonData()\n","        # 웹툰 정보 수집 시작\n","        idx = 0\n","        for i in range(len(thumb)):\n","            sleep(0.5)\n","            print(\"\\rprocess(finish)): \" + str(i + 1) + \" / \" + str(len(thumb)), end=\"\")\n","            # 첫 번째 웹툰부터 순서대로 클릭\n","            page = driver.find_elements(By.CLASS_NAME, \"thumb\")[1:]\n","            page[i].click()\n","\n","            # 이동한 페이지 주소 읽고 파싱\n","            html = driver.page_source\n","            soup = bs(html, 'html.parser')\n","\n","            # 정보 수집\n","            day = \"완결\"\n","            title = soup.find('span', {'class': 'title'}).text\n","            image_url = soup.find('div', {'class': 'thumb'}).find('a').find('img')\n","            image_url = image_url['src']\n","            author = soup.find('span', {'class': 'wrt_nm'}).text[8:]\n","            author = author.replace(' / ', ', ')\n","            genre = soup.find('span', {'class': 'genre'}).text.split(\", \")\n","            story = soup.find('div', {'class': 'detail'}).find('p').text\n","\n","            # 리스트에 추가\n","            part_wd.id_list.append(idx)\n","            part_wd.thumbnail_list.append(image_url)\n","            part_wd.title_list.append(title)\n","            part_wd.author_list.append(author)\n","            part_wd.day_list.append(day)\n","            if genre[1] == \"무협/사극\":\n","                part_wd.genre_list.append(\"무협\")\n","            else:\n","                part_wd.genre_list.append(genre[1])\n","            part_wd.story_list.append(story)\n","            part_wd.platform_list.append(\"네이버\")\n","            part_wd.url_list.append(driver.current_url)\n","\n","            # 뒤로 가기\n","            idx += 1\n","            driver.back()\n","            sleep(0.5)\n","        return part_wd\n","\n","    # 네이버 웹툰 각각의 정보 가져오기\n","    def get_naver_webtoon_info(self):\n","        wd = WebtoonData()\n","\n","        if os.path.isfile(\"naver1.csv\"):\n","            first_td = ut.get_from_csv(\"naver1.csv\")\n","        else:\n","            first_wd = self.get_weekday_info()\n","            first_td = td.make_total_data(first_wd)\n","            ut.make_csv(\"naver1.csv\", first_td)\n","\n","        if os.path.isfile(\"naver2.csv\"):\n","            second_td = ut.get_from_csv(\"naver2.csv\")\n","        else:\n","            second_wd = self.get_finish_info()\n","            second_td = td.make_total_data(second_wd)\n","            ut.make_csv(\"naver2.csv\", second_td)\n","\n","\n","        total_td = pd.concat([first_td, second_td])\n","        total_td['id'] = [i for i in range(len(total_td))]\n","        total_td.set_index('id', inplace=True)\n","\n","        print()\n","        return total_td\n","\n","    # 카카오 웹툰 각각의 정보 가져오고 파일로까지 저장하기 (요일 단위로)\n","    def get_kakao_webtoon_info(self):\n","        driver = webdriver.Chrome('chromedriver', options=options)\n","        action = ActionChains(driver)\n","        driver.get(self.kw_url)\n","        sleep(3)\n","        # 로그인 해야 들어갈 수 있는 것들 때문에 일단 로그인하기\n","        self.login_on_kakao_page(driver)\n","\n","        # # 완결 웹툰은 일단 제외하고 요일별 페이지 가져오기\n","        # days = driver.find_elements(By.CLASS_NAME, \"e1201h8a0\")[:-1]\n","        # 완결 웹툰 포함해서 요일별 페이지 가져오기\n","        days = driver.find_elements(By.CLASS_NAME, \"e1201h8a0\")\n","\n","        day_tds = []\n","        filenames = []\n","\n","        idx = 0\n","        for i in range(len(days)):\n","            filename = \"kakao\" + str(i) + \".csv\"\n","            filenames.append(filename)\n","            if os.path.isfile(filename):\n","                day_tds.append(ut.get_from_csv(filename))\n","                idx += len(day_tds[-1])\n","                continue\n","\n","            day_wd = WebtoonData()\n","            total_titles = []\n","\n","            # 요일별 페이지에 있는 웹툰들 가져오기 (스크롤해야 보이는 것 까지 포함)\n","            day = driver.find_elements(By.CLASS_NAME, \"e1201h8a0\")[i]\n","            action.move_to_element(day).click().perform()\n","            if i == 7:\n","                self.do_scroll_down(80, driver)\n","            else:\n","                self.do_scroll_down(10, driver)\n","            webtoons = driver.find_elements(By.CLASS_NAME, \"css-qm6qod\")\n","\n","            # 웹툰별로 정보 저장하기\n","            for j in range(len(webtoons)):\n","                print(\"\\rday[\" + str(i) + \"] - process: \" + str(j + 1) + \" / \" + str(len(webtoons)), end=\"\")\n","                # 해당 웹툰으로 이동하기\n","                webtoon = driver.find_elements(By.CLASS_NAME, \"css-qm6qod\")[j]\n","                action.move_to_element(webtoon).key_down(Keys.CONTROL).click().key_up(Keys.CONTROL).perform()\n","                sleep(2)\n","                driver.switch_to.window(driver.window_handles[1])\n","\n","                # 이미지 정보 먼저 저장하기\n","                html = driver.page_source\n","                soup = bs(html, 'html.parser')\n","                image_url = soup.find('div', {'class': 'css-1y42t5x'}).find('img')\n","                image_url = image_url['src']\n","                image_url = \"https:\" + image_url\n","\n","                # 작품소개 창 열기\n","                notice = driver.find_elements(By.CLASS_NAME, \"jsx-3114325382\")\n","                if notice:\n","                    notice[0].click()\n","                driver.find_element(By.CLASS_NAME, \"css-nxuz68\").click()\n","\n","                # 현재 창에서 데이터 읽기\n","                sleep(0.5)\n","                html = driver.page_source\n","                soup = bs(html, 'html.parser')\n","\n","                title = soup.find('h2', {'class': 'css-jgjrt'}).text\n","                day = soup.find_all('div', {'class': 'css-7a7cma'})[0].text\n","                day_word_end_idx = day.find(\" 연재\")\n","                if day_word_end_idx == -1:\n","                    day = \"완결\"\n","                else:\n","                    day = day[:day_word_end_idx]\n","                author = soup.find_all('div', {'class': 'css-7a7cma'})[1].text\n","                author = author.replace(',', ', ')\n","                genre = soup.find('div', {'class': 'infoBox'})\n","                genre = genre.find_all('div', {'class': 'jsx-3755015728'})[2].text\n","                genre = genre[genre.find(\"웹툰\") + 2:]\n","                story = soup.find('div', {'class': 'descriptionBox'}).text\n","\n","                # 다른 요일에서 이미 추가된거면 스킵하기\n","                if title in total_titles:\n","                    continue\n","\n","                # 리스트에 추가\n","                day_wd.id_list.append(idx)\n","                day_wd.thumbnail_list.append(image_url)\n","                day_wd.title_list.append(title)\n","                day_wd.author_list.append(author)\n","                day_wd.day_list.append(day)\n","                if genre == \"액션무협\":\n","                    day_wd.genre_list.append(\"무협\")\n","                else:\n","                    day_wd.genre_list.append(genre)\n","                day_wd.story_list.append(story)\n","                day_wd.platform_list.append(\"카카오\")\n","                day_wd.url_list.append(driver.current_url)\n","                total_titles.append(title)\n","\n","                idx += 1\n","                # 다시 메인페이지로 돌아가기\n","                driver.close()\n","                driver.switch_to.window(driver.window_handles[0])\n","                sleep(0.5)\n","\n","            day_tds.append(td.make_total_data(day_wd))\n","            ut.make_csv(filename, day_tds[-1])\n","\n","        # 요일별로 만든 dataframe 모두 합치고 기존 것들은 지우기\n","        total_td = pd.concat(day_tds)\n","        total_td = total_td.drop_duplicates(['title'])\n","        total_td['id'] = [i for i in range(len(total_td))]\n","        total_td.set_index('id', inplace=True)\n","\n","        # for filename in filenames:\n","        #     ut.delete_csv(filename)\n","\n","        print()\n","        return total_td\n","\n","    # 카카오 페이지에 로그인 하기\n","    def login_on_kakao_page(self, driver):\n","        # 로그인 페이지로 이동하기\n","        driver.find_elements(By.CLASS_NAME, 'css-vurnku')[1].click()\n","        sleep(1)\n","        driver.switch_to.window(driver.window_handles[1])\n","        # 아이디 및 패스워드 입력하고 로그인 누르기\n","        driver.find_element(By.NAME, 'email').send_keys(self.login_id)\n","        driver.find_element(By.NAME, 'password').send_keys(self.login_pw)\n","        driver.find_element(By.CLASS_NAME, 'btn_confirm').click()\n","        sleep(1)\n","        driver.switch_to.window(driver.window_handles[0])\n","        sleep(1)\n","\n","    # 스크롤을 내려야 나오는 데이터를 얻기 위해 스크롤하기\n","    def do_scroll_down(self, seconds, driver):\n","        start = datetime.datetime.now()\n","        end = start + datetime.timedelta(seconds=seconds)\n","        while True:\n","            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)\n","            sleep(0.5)\n","            if datetime.datetime.now() > end:\n","                break\n"]},{"cell_type":"markdown","source":["# myTokenize"],"metadata":{"id":"_bHKw8_sKhLZ"}},{"cell_type":"code","source":["from konlpy.tag import Kkma\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","class MyTokenize:\n","    pos = (\"N\", \"V\", \"M\", \"XR\")  # 토큰화할 단어 태그 http://kkma.snu.ac.kr/documents/?doc=postag\n","\n","    def __init__(self, data):\n","        self.tfidf_vectorizer = TfidfVectorizer(\n","            tokenizer=self.tokenizer,  # 문장에 대한 tokenizer (위에 정의한 함수 이용)\n","            min_df=1,  # 단어가 출현하는 최소 문서의 개수\n","            sublinear_tf=True  # tf값에 1+log(tf)를 적용하여 tf값이 무한정 커지는 것을 막음\n","        )\n","        self.story_data = data['story']\n","\n","    # 토큰화하기\n","    def tokenizer(self, raw_texts):\n","        kkma = Kkma()\n","        p = kkma.pos(raw_texts)\n","        o = [word for word, tag in p if (len(word) > 1) and (tag.startswith(self.pos))]\n","        return o\n","\n","    # 토큰화 결과 확인해보기\n","    def print_tokenize_result(self):\n","        tokenized_sentence = []\n","        for i, story in enumerate(self.story_data):\n","            tokenized_sentence.append(self.tokenizer(story))\n","            print(str(i+1) + \" / \" + str(len(self.story_data)))\n","            print(story)\n","            print(tokenized_sentence[-1])\n","            print(\"------------------------------------------------------------\")\n","\n","    # 토근화를 기반으로 벡터화하고 데이터 반환하기\n","    def get_vectorized_data(self):\n","        vectorized = self.tfidf_vectorizer.fit_transform(self.story_data)\n","        return vectorized\n"],"metadata":{"id":"tQxwIP-FLXu2","executionInfo":{"status":"ok","timestamp":1669463919157,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# myStoryClustering"],"metadata":{"id":"K6C-2d7AKnoB"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import font_manager, rc\n","from sklearn.decomposition import TruncatedSVD\n","import seaborn as sns\n","\n","\n","class MyStoryClustering:\n","    top_n_features = 5\n","\n","    def __init__(self, vectorized, vectorizer, total_data):\n","        self.vectorized = vectorized\n","        self.vectorizer = vectorizer\n","        self.data = total_data\n","        self.kmeans = KMeans()\n","\n","        # 한글 폰트 설정\n","        rc('font', family='NanumBarunGothic') \n","\n","    # silhoutte 방법으로 적정 k값 구하기\n","    def get_proper_k(self, data_index):\n","        max_k = len(data_index) // 10\n","\n","        # 데이터 개수가 너무 적으면 k=1로 하기\n","        if max_k <= 1:\n","            proper_k = 1\n","        else:\n","            silhoutte_values = []\n","            for i in range(2, max_k+1):\n","                kmeans = KMeans(n_clusters=i, init='k-means++')\n","                pred = kmeans.fit_predict(self.vectorized[data_index])\n","                silhoutte_values.append(np.mean(silhouette_samples(self.vectorized[data_index], pred)))\n","\n","            proper_k = np.argmax(silhoutte_values) + 2\n","\n","        return proper_k\n","\n","    # K-means로 군집화시키기\n","    def kmeans_cluster(self, genre, data_index, k=-1):\n","        if k == -1:\n","            cluster_num = self.get_proper_k(data_index)\n","        else:\n","            cluster_num = k\n","        print(genre + \": \" + str(cluster_num))\n","        self.kmeans = KMeans(n_clusters=cluster_num)\n","        cluster_label = self.kmeans.fit_predict(self.vectorized[data_index])\n","        return cluster_label\n","\n","    # 군집별 핵심단어 추출하기\n","    def get_cluster_details(self, genre):\n","        # 각 클러스터별 핵심 단어를 저장할 변수\n","        cluster_details = pd.DataFrame({\n","            \"genre\": [],\n","            \"cluster_num\": [],\n","            \"words\": [],\n","        })\n","\n","        feature_names = self.vectorizer.get_feature_names_out()\n","\n","        # 각 클러스터 레이블별 feature들의 center값들 내림차순으로 정렬 후의 인덱스를 반환\n","        center_feature_idx = self.kmeans.cluster_centers_.argsort()[:, ::-1]\n","\n","        # 각 feature별 center값들 정렬한 인덱스 중 상위 값들 추출\n","        top_features = []\n","        for cluster_num in range(len(center_feature_idx)):\n","            top_feature_idx = center_feature_idx[cluster_num, :self.top_n_features]\n","            top_feature = [feature_names[idx] for idx in top_feature_idx]\n","            top_features.append(top_feature)\n","\n","        cluster_details['genre'] = [genre for _ in range(len(center_feature_idx))]\n","        cluster_details['cluster_num'] = range(len(center_feature_idx))\n","        cluster_details['words'] = top_features\n","\n","        return cluster_details\n","\n","    # 같은 클러스터 그룹이 아니더라도 거리순으로 가까운 12개의 웹툰 인덱스 반환하기\n","    def get_similarity_without_group(self):\n","        dists = distance.squareform(distance.pdist(self.vectorized.toarray()))\n","        return [dists[i].argsort()[1:13] for i in range(len(self.data))]\n","\n","    # 같은 클러스터 그룹 내에서 가까운 웹툰 인덱스 반환하기 (+ 시각화하기)\n","    def get_similarity(self, cluster):\n","        self.data = self.data.sort_index()\n","\n","        indexes = []\n","        for idx, row in td.total_data.iterrows():\n","            # 해당 행의 웹툰이 어느 클러스터에 속해있고 인덱스는 몇인지 구하기\n","            target_cluster = row[cluster]\n","            target_webtoon_idx = idx\n","            target_genre = row[\"genre\"]\n","\n","            # 해당 클러스트 안에 있는 웹툰들을 모두 구하기\n","            if target_cluster == \"cluster_story_in_genre\":\n","                webtoons_in_target_cluster = self.data[(self.data[cluster] == target_cluster) & (self.data['genre'] == target_genre)]\n","            else:\n","                webtoons_in_target_cluster = self.data[self.data[cluster] == target_cluster]\n","            webtoons_idx = webtoons_in_target_cluster.index\n","            \n","            similarity = cosine_similarity(self.vectorized[target_webtoon_idx], self.vectorized[webtoons_idx]) # 위에서 추출한 카테고리로 클러스터링된 문서들의 인덱스 중 비교기준문서를 제외한 다른 문서들과의 유사도 측정\n","            sorted_idx = np.argsort(similarity)[:, ::-1] # array 내림차순으로 정렬한 후 인덱스 반환\n","            sorted_idx = sorted_idx[:, 1:] # 비교문서 당사자는 제외한 인덱스 추출 (내림차순 정렬했기때문에 0번째가 무조건 가장 큰 값임)\n","            sorted_idx = sorted_idx.reshape(-1) # index로 넣으려면 1차원으로 reshape해주기\n","            sorted_sim_values = similarity.reshape(-1)[sorted_idx] # 앞에서 구한 인덱스로 유사도 행렬값도 정렬\n","            indexes.append(list(webtoons_idx[sorted_idx[:len([x for x in sorted_sim_values if x != 0])]])) # 높은 유사도 순으로 웹툰 인덱스 반환 (0인 것은 뺌)\n","\n","            # if idx == 0:\n","            #     print(\"타겟 클러스터 번호:\", target_cluster)\n","            #     print(\"타겟 웹툰 인덱스:\", target_webtoon_idx)\n","            #     print(\"유사도 비교 기준 웹툰:\", row['title'])\n","            #     print(\"유사한 웹툰 인덱스:\")\n","            #     print(list(webtoons_idx))\n","            #     print(\"유사도(내림차순 정렬):\")\n","            #     print(sorted_sim_values)\n","            #     print(len(sorted_sim_values))\n","            #     print(len(webtoons_idx[sorted_idx[:len([x for x in sorted_sim_values if x != 0])]]))\n","            #     print()\n","            #\n","            #     # 그래프 생성\n","            #     selected_sim_df = pd.DataFrame()\n","            #     selected_sim_df['title'] = webtoons_in_target_cluster.iloc[sorted_idx]['title']\n","            #     selected_sim_df['similarity'] = sorted_sim_values\n","            #\n","            #     plt.figure(figsize=(25, 10), dpi=60)\n","            #     sns.barplot(data=selected_sim_df, x='similarity', y='title')\n","            #     plt.title(row['title'])\n","            #     plt.show()\n","\n","        return indexes"],"metadata":{"id":"pEmf7CEALbhj","executionInfo":{"status":"ok","timestamp":1669463919157,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# FeatureExtractionByStyleTransfer"],"metadata":{"id":"-A-4djgyKsGb"}},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","import tensorflow as tf\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics.pairwise import cosine_similarity\n","import matplotlib.pyplot as plt"],"metadata":{"id":"Myzsfa2J6w-i","executionInfo":{"status":"ok","timestamp":1669463919158,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# 스타일 추출하는 모델 정의하기\n","class StyleContentModel(tf.keras.models.Model):\n","    def __init__(self, style_layers, content_layers):\n","        super(StyleContentModel, self).__init__()\n","        self.vgg = self.vgg_layers(style_layers + content_layers)\n","        self.style_layers = style_layers\n","        self.content_layers = content_layers\n","        self.num_style_layers = len(style_layers)\n","        self.vgg.trainable = False\n","\n","    # vgg 모델 불러오기\n","    def vgg_layers(self, layer_names):\n","        # 이미지넷 데이터셋에 사전학습된 VGG 모델 불러오기\n","        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","        vgg.trainable = False\n","\n","        outputs = [vgg.get_layer(name).output for name in layer_names]\n","\n","        model = tf.keras.Model([vgg.input], outputs)\n","        return model\n","\n","    # 스타일을 뽑아내기 위한 그람 행렬\n","    def gram_matrix(self, input_tensor):\n","        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n","        input_shape = tf.shape(input_tensor)\n","        num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n","        return result / num_locations\n","\n","    def call(self, inputs):\n","        # \"[0,1] 사이의 실수 값을 입력으로 받습니다\"\n","        inputs = inputs * 255.0\n","        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","        outputs = self.vgg(preprocessed_input)\n","        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n","                                          outputs[self.num_style_layers:])\n","\n","        style_outputs = [self.gram_matrix(style_output)\n","                         for style_output in style_outputs]\n","\n","        content_dict = {content_name: value\n","                        for content_name, value\n","                        in zip(self.content_layers, content_outputs)}\n","\n","        style_dict = {style_name: value\n","                      for style_name, value\n","                      in zip(self.style_layers, style_outputs)}\n","\n","        return {'content': content_dict, 'style': style_dict}"],"metadata":{"id":"LDucEqoW62D-","executionInfo":{"status":"ok","timestamp":1669463919158,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class FeatureExtractionByStyleTransfer:\n","    content_layers = ['block5_conv2']\n","    style_layers = ['block1_conv1',\n","                    'block2_conv1',\n","                    'block3_conv1',\n","                    'block4_conv1',\n","                    'block5_conv1']\n","\n","    def __init__(self, data):\n","        self.data = data\n","        self.style_info_list = np.array([])\n","\n","    # 학습을 시키면서 스타일 추출해내고 이미지로 결과 보여주기\n","    def extract_and_show_style(self, titles, images, train_n):\n","        style_images = []\n","        for title in titles:\n","            index = td.title_to_index(title)\n","            style_images.append(images[index])\n","\n","        content_image = np.ones((1, 100, 100, 3))\n","        extractor = StyleContentModel(self.style_layers, self.content_layers)\n","        tf_image = tf.Variable(style_images[0][:, :self.min_dim, :self.min_dim, :])\n","        opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n","\n","        trained_images = []\n","        for i, style_image in enumerate(style_images):\n","            style_targets = extractor.call(style_image)['style']\n","            content_targets = extractor.call(content_image)['content']\n","            tf_image.assign(content_image)\n","\n","            squeezed_img = np.array(style_image).squeeze()\n","            plt.subplot(1, 2, 1)\n","            plt.imshow(squeezed_img)\n","            plt.title(\"target\")\n","\n","            for step in range(train_n):\n","                print(\"\\r\" + str(step + 1) + \"/\" + str(train_n), end=\"\")\n","                self.train_step(image=tf_image,\n","                                style_targets=style_targets,\n","                                content_targets=content_targets,\n","                                opt=opt,\n","                                extractor=extractor)\n","\n","            result_image = np.array(tf_image).squeeze()\n","            trained_images.append(result_image)\n","            \n","            plt.subplot(1, 2, 2)\n","            plt.imshow(result_image)\n","            plt.title(\"result\")\n","\n","            plt.show()\n","        \n","        return trained_images\n","\n","    # 학습을 시키면서 스타일 추출해내기\n","    def extract_style(self, style_images, train_n):\n","        content_image = np.ones((1, 100, 100, 3))\n","        extractor = StyleContentModel(self.style_layers, self.content_layers)\n","        tf_image = tf.Variable(style_images[0][:, :self.min_dim, :self.min_dim, :])\n","        opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n","\n","        trained_images = []\n","        for i, style_image in enumerate(style_images):\n","            print(\"\\r\" + str(i + 1) + \"/\" + str(len(style_images)), end=\"\")\n","            style_targets = extractor.call(style_image)['style']\n","            content_targets = extractor.call(content_image)['content']\n","            tf_image.assign(content_image)\n","\n","            for step in range(train_n):\n","                self.train_step(image=tf_image,\n","                                style_targets=style_targets,\n","                                content_targets=content_targets,\n","                                opt=opt,\n","                                extractor=extractor)\n","\n","            result_image = np.array(tf_image).squeeze()\n","            trained_images.append(result_image)\n","\n","        return trained_images\n","\n","    # 스타일 학습시킬 때 쓸 loss 함수\n","    def style_content_loss(self, outputs, style_targets, content_targets):\n","        style_weight = 1e4\n","        content_weight = 1e-2\n","\n","        style_outputs = outputs['style']\n","        content_outputs = outputs['content']\n","        style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2)\n","                               for name in style_outputs.keys()])\n","        style_loss *= style_weight / len(self.style_layers)\n","\n","        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2)\n","                                 for name in content_outputs.keys()])\n","        content_loss *= content_weight / len(self.content_layers)\n","        loss = style_loss + content_loss\n","        return loss\n","\n","    # 픽셀 값이 실수이므로 0과 1 사이의 값으로 바꾸기\n","    def clip_0_1(self, image):\n","        return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n","\n","    # 학습하기\n","    @tf.function()\n","    def train_step(self, image, style_targets, content_targets, opt, extractor):\n","        with tf.GradientTape() as tape:\n","            outputs = extractor(image)\n","            loss = self.style_content_loss(outputs=outputs,\n","                                           style_targets=style_targets,\n","                                           content_targets=content_targets)\n","\n","\n","        grad = tape.gradient(loss, image)\n","        opt.apply_gradients([(grad, image)])\n","        image.assign(self.clip_0_1(image))\n","\n","    # tensor 자료형을 이미지로 변환해주기\n","    def tensor_to_image(self, tensor):\n","        tensor = tensor * 255\n","        tensor = np.array(tensor, dtype=np.uint8)\n","        if np.ndim(tensor) > 3:\n","            assert tensor.shape[0] == 1\n","            tensor = tensor[0]\n","        return PIL.Image.fromarray(tensor)\n","\n","    # 이미지 plt로 보여주기\n","    def imshow(self, image, title=None):\n","        # self.tensor_to_image(result_image)\n","        # plt.subplot(1, 2, 2)\n","        # self.imshow(result_image, \"After\")\n","        # plt.show()\n","        if len(image.shape) > 3:\n","            image = tf.squeeze(image, axis=0)\n","\n","        plt.imshow(image)\n","        if title:\n","            plt.title(title)\n","\n","    # 유사도 그래프로 비교해보기\n","    def compare_similarity(self, idx, row):\n","        self.data = self.data.sort_index()\n","\n","        # 해당 행의 웹툰이 어느 클러스터에 속해있고 인덱스는 몇인지 구하기\n","        target_cluster = row[\"cluster_style\"]\n","        target_webtoon_idx = idx\n","\n","        # 해당 클러스트 안에 있는 웹툰들을 모두 구하기\n","        webtoons_in_target_cluster = self.data[self.data[\"cluster_style\"] == target_cluster]\n","\n","        webtoons_idx = webtoons_in_target_cluster.index\n","\n","        # 위에서 추출한 카테고리로 클러스터링된 문서들의 인덱스 중 비교기준문서를 제외한 다른 문서들과의 유사도 측정\n","        similarity = cosine_similarity(self.style_info_list[target_webtoon_idx].reshape(1, -1), self.style_info_list[webtoons_idx])\n","\n","        # array 내림차순으로 정렬한 후 인덱스 반환\n","        sorted_idx = np.argsort(similarity)[:, ::-1]\n","        # 비교문서 당사자는 제외한 인덱스 추출 (내림차순 정렬했기때문에 0번째가 무조건 가장 큰 값임)\n","        sorted_idx = sorted_idx[:, 1:]\n","\n","        # index로 넣으려면 1차원으로 reshape해주기\n","        sorted_idx = sorted_idx.reshape(-1)\n","\n","        # 앞에서 구한 인덱스로 유사도 행렬값도 정렬\n","        sorted_sim_values = similarity.reshape(-1)[sorted_idx]\n","\n","        # 높은 유사도 순으로 웹툰 인덱스 반환 (0인 것은 뺌)\n","        return list(webtoons_idx[sorted_idx[:len([x for x in sorted_sim_values if x != 0])]])"],"metadata":{"id":"WYZrx5poLi_s","executionInfo":{"status":"ok","timestamp":1669463919158,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# FeatureExtractionByHaralick\n","---\n","- https://hmkim312.github.io/posts/Mahotas%EB%A1%9C_%ED%95%B4%EB%B3%B4%EB%8A%94_%EB%B9%84%EC%8A%B7%ED%95%9C_%EC%9D%B4%EB%AF%B8%EC%A7%80_%EC%B0%BE%EA%B8%B0/"],"metadata":{"id":"crCDuuT97PPV"}},{"cell_type":"code","source":["import mahotas as mh\n","import numpy as np\n","import os\n","from sklearn.preprocessing import StandardScaler\n","\n","class FeatureExtractionByHaralick:\n","    def __init__(self, folder):\n","        self.folder = folder\n","        self.features = []\n","        self.file_lst = [data_path+folder+'/thumbnail'+str(i)+\".jpg\" for i in range(len(os.listdir(data_path+folder)))]\n","\n","        for i, im in enumerate(self.file_lst):\n","            im = mh.imread(im)\n","            im = mh.colors.rgb2gray(im, dtype = np.uint8)\n","            self.features.append(mh.features.haralick(im).ravel())\n","\n","        sc = StandardScaler()\n","        self.features = sc.fit_transform(self.features)\n","\n","    def get_features(self):\n","        return self.features"],"metadata":{"id":"hpQT6rOV7Esz","executionInfo":{"status":"ok","timestamp":1669463919158,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["# Clustering"],"metadata":{"id":"ovNS0GVM5x3d"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import TruncatedSVD\n","from scipy.spatial import distance\n","\n","class Clustering():\n","\n","    def __init__(self, datas):\n","        self.datas = datas\n","        self.pred = []\n","        self.dists = []\n","\n","    # k-means 클러스터링 하기\n","    def kmeans_cluster(self, k):\n","        kmeans = KMeans(n_clusters=k)\n","        self.pred = kmeans.fit_predict(self.datas)\n","\n","    # 차원 축소하기\n","    def svd(self, svd_n):\n","        self.datas = np.array([x.reshape(-1) for x in self.datas])\n","        if svd_n != 0:\n","            svd = TruncatedSVD(n_components=svd_n)\n","            self.datas = np.array(svd.fit_transform(self.datas))\n","\n","    # 클러스터링 결과 그래프로 보여주기\n","    def visualize(self):\n","        fig = plt.figure(figsize=(10,10))\n","        colors = plt.cm.get_cmap(\"rainbow\")(np.linspace(0, 1, len(set(self.pred))))\n","        ax = fig.add_subplot(1, 1, 1)\n","\n","        for k, col in zip(range(len(colors)), colors):\n","            my_members = (self.pred == k)\n","            ax.plot(\n","                self.datas[my_members, 0],\n","                self.datas[my_members, 1],\n","                'w',\n","                markerfacecolor=col,\n","                marker='.',\n","                markersize=12\n","            )\n","        ax.set_title('K-Means')\n","\n","        plt.show()\n","\n","    # 거리 계산하기\n","    def calculate_dists(self):\n","        self.dists = distance.squareform(distance.pdist(self.datas))\n","\n","    # 해당 이미지와 가까운 거리에 있는 순으로 이미지 인덱스 배열 가져오기\n","    def get_close_images_idx(self, image_n, length):\n","        my_dist = self.dists[image_n].argsort()[1:length+1]\n","        return list(my_dist)\n","\n","    # n번째 이미지와 m번째로 거리가 가까운 이미지 선택하기\n","    def selectimage(self, n, m, file_lst):\n","        data_position = self.dists[n].argsort()[m]\n","        image = mh.imread(file_lst[data_position])\n","        return data_position, image\n","\n","    # 거리가 가까운 순으로 8가지 이미지 보여주기 (자신 포함)\n","    def plotImages(self, title, folder):\n","        file_lst = [data_path+folder+'/thumbnail'+str(i)+\".jpg\" for i in range(len(os.listdir(data_path+folder)))]\n","        n = td.title_to_index(title)\n","        if folder == \"style_images\":\n","            fig, ax = plt.subplots(4,4, figsize = (10,10))\n","            fig.suptitle('style images')\n","            for i in range(8):\n","                index, image = self.selectimage(n,i,file_lst)\n","                ax[(i//4)*2][i%4].imshow(image)\n","                ax[(i//4)*2][i%4].set_xticks([])\n","                ax[(i//4)*2][i%4].set_yticks([])\n","\n","                file_name = data_path+'resized_images'+'/thumbnail'+str(index)+'.jpg'\n","                ax[(i//4)*2+1][i%4].imshow(mh.imread(file_name))\n","                ax[(i//4)*2+1][i%4].set_xticks([])\n","                ax[(i//4)*2+1][i%4].set_yticks([])\n","                print(td.index_to_title(index))\n","        else:\n","            fig, ax = plt.subplots(2,4, figsize = (10,5))\n","            fig.suptitle('origin images')\n","            for i in range(8):\n","                index, image = self.selectimage(n,i,file_lst)\n","                ax[i//4][i%4].imshow(image)\n","                ax[i//4][i%4].set_xticks([])\n","                ax[i//4][i%4].set_yticks([])\n","                print(td.index_to_title(index))\n","        plt.show()"],"metadata":{"id":"x-_-Bov_5tOm","executionInfo":{"status":"ok","timestamp":1669464204359,"user_tz":-540,"elapsed":375,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"CJ4PyYGeLoZI"}},{"cell_type":"code","source":["import os.path\n","import pandas as pd\n","\n","\n","naver_csv_filename = data_path+\"/네이버웹툰정보.csv\"\n","kakao_csv_filename = data_path+\"카카오웹툰정보.csv\"\n","cluster_csv_filename = data_path+\"클러스터정보.csv\"\n","cluster_detail_csv_filename = data_path+\"클러스터상위단어.csv\"\n","vector_filename = data_path+\"vector_data.pickle\"\n","images_filename = data_path+\"images.pickle\"\n","trained_images_filename = data_path+\"trained_images.pickle\"\n","image_haralick_features_filename = data_path+\"image_haralick_features.pickle\"\n","style_image_haralick_features_filename = data_path+\"style_image_haralick_features.pickle\""],"metadata":{"id":"eNwor_LxjmJv","executionInfo":{"status":"ok","timestamp":1669464210405,"user_tz":-540,"elapsed":386,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# 네이버 및 카카오 웹툰 크롤링 하기 (새로 하기 또는 저장된 데이터 불러오기)\n","def do_web_crawling():\n","    wc = MyWebCrawling()\n","\n","    # 네이버 웹툰 정보 가져오기\n","    print(\"--naver webtoon crawling start--\")\n","    if not os.path.isfile(naver_csv_filename):\n","        naver_td = wc.get_naver_webtoon_info()\n","        ut.make_csv(naver_csv_filename, naver_td)\n","    else:\n","        naver_td = ut.get_from_csv(naver_csv_filename)\n","    print(\"--naver webtoon crawling end--\")\n","\n","\n","    # 카카오 웹툰 정보 가져오기\n","    print(\"--kakao webtoon crawling start--\")\n","    if not os.path.isfile(kakao_csv_filename):\n","        # 데이터 양이 많아서 요일별로 끊어서 파일 만들고 합치기\n","        kakao_td = wc.get_kakao_webtoon_info()\n","        ut.make_csv(kakao_csv_filename, kakao_td)\n","    else:\n","        kakao_td = ut.get_from_csv(kakao_csv_filename)\n","    print(\"--kakao webtoon crawling end--\")\n","\n","\n","    # 가져온 데이터들 합치기\n","    td.merge_total_data([naver_td, kakao_td])\n","    # td.total_data = naver_td\n","    # td.total_data = kakao_td\n","    # 웹툰 카테고리 분류하기\n","    td.save_category()"],"metadata":{"id":"G9LDvdNpjn8z","executionInfo":{"status":"ok","timestamp":1669464211044,"user_tz":-540,"elapsed":3,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# 토큰화 및 벡터화하기 (새로 하기 또는 저장된 데이터 불러오기)\n","def do_tokenize_and_vectorize():\n","    print(\"--vectorized start--\")\n","    if not os.path.isfile(vector_filename):\n","        tk = MyTokenize(td.total_data)\n","        vectorized = tk.get_vectorized_data()\n","        vectorizer = tk.tfidf_vectorizer\n","        ut.save_data(vector_filename, (vectorized, vectorizer))\n","    else:\n","        vectorized, vectorizer = ut.load_data(vector_filename)\n","    print(\"--vectorized end--\")\n","    story_ct = MyStoryClustering(vectorized, vectorizer, td.total_data)\n","    return story_ct"],"metadata":{"id":"c2qloJPCjrMG","executionInfo":{"status":"ok","timestamp":1669464211045,"user_tz":-540,"elapsed":4,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# story에 대한 k-means 클러스터링 하기\n","def do_clustering_by_story(story_ct, k_for_total=175):\n","    print(\"--kmeans story clustering start--\")\n","    print(\"\\n<적정 k값>\")\n","\n","    cluster_details_list = []\n","\n","    # 전체 웹툰 안에서 클러스터링 하기\n","    total_index = list(range(len(td.total_data)))\n","    cluster_labels_for_whole = story_ct.kmeans_cluster(\"전체\", total_index, k=k_for_total)\n","    cluster_details = story_ct.get_cluster_details(\"전체\")\n","    cluster_details_list.append(cluster_details)\n","    # story_ct.visualize(cluster_labels_for_whole)\n","\n","    # 각 장르 안에서 클러스터링 하기\n","    cluster_labels_for_genre = [-1 for _ in range(len(td.total_data))]\n","    for genre in td.categories:\n","        current_data_index = td.total_data.index[td.total_data['genre'] == genre].tolist()\n","        current_data_index = list(map(int, current_data_index))\n","        cluster_label = story_ct.kmeans_cluster(genre, current_data_index)\n","        for i in range(len(current_data_index)):\n","            cluster_labels_for_genre[current_data_index[i]] = cluster_label[i]\n","        cluster_details = story_ct.get_cluster_details(genre)\n","        cluster_details_list.append(cluster_details)\n","\n","    print()\n","    print(\"--kmeans story clustering end--\")\n","\n","    # 결과 저장하기\n","    print(\"--save result data start--\")\n","    td.total_data[\"cluster_story\"] = cluster_labels_for_whole\n","    td.total_data[\"cluster_story_in_genre\"] = cluster_labels_for_genre\n","    td.cluster_details = pd.concat(cluster_details_list)\n","    td.total_data[\"cluster_story_group\"] = story_ct.get_similarity(\"cluster_story\")\n","    td.total_data[\"cluster_story_group_in_genre\"] = story_ct.get_similarity(\"cluster_story_in_genre\")\n","\n","    print(\"--save result data end--\")"],"metadata":{"id":"ENO0dCdqjvtz","executionInfo":{"status":"ok","timestamp":1669464211045,"user_tz":-540,"elapsed":3,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# style에 대한 k-means 클러스터링 하기\n","def do_clustering_by_style(k, style_train_n, svd_n, clustering_type):\n","    style_ct = FeatureExtractionByStyleTransfer(td.total_data)\n","\n","    # 이미지 url로부터 폴더로 저장 -> 이미지 폴더로부터 로딩 (새로 하기 또는 저장된 데이터 불러오기)\n","    ut.save_images(td.total_data['thumbnail'])\n","    thumbnails = ut.get_imges()\n","\n","\n","    print(\"--style extraction start--\")\n","    # 스타일 추출 테스트\n","    # titles = [\"독립일기\", \"대학일기\", \"수능일기\", \"호랑신랑뎐\", \"앵무살수\", \"전지적 독자 시점\"]\n","    # trained_images = style_ct.extract_and_show_style(titles, thumbnails, 1000)\n","\n","    # 훈련으로 스타일 추출하거나 이미 추출한 이미지 가져오기\n","    if not os.path.isfile(trained_images_filename):\n","        trained_images = style_ct.extract_style(thumbnails, style_train_n)\n","        ut.save_data(trained_images_filename, trained_images)\n","    else:\n","        trained_images = ut.load_data(trained_images_filename)\n","    ut.save_style_images(trained_images)\n","    print(\"--style extraction end--\")\n","\n","\n","    print(\"--haralick style clustering start--\")\n","    # 기본 이미지에 대해 haralick으로 뽑아낸 features 가져오기\n","    if not os.path.isfile(image_haralick_features_filename):\n","        haralick = FeatureExtractionByHaralick(\"resized_images\")\n","        image_features = haralick.get_features()\n","        ut.save_data(image_haralick_features_filename, image_features)\n","    else:\n","        image_features = ut.load_data(image_haralick_features_filename)\n","\n","    # 스타일 이미지에 대해 haralick으로 뽑아낸 features 가져오기\n","    if not os.path.isfile(style_image_haralick_features_filename):\n","        haralick = FeatureExtractionByHaralick(\"style_images\")\n","        style_image_features = haralick.get_features()\n","        ut.save_data(style_image_haralick_features_filename, style_image_features)\n","    else:\n","        style_image_features = ut.load_data(style_image_haralick_features_filename)\n","    print(\"--haralick style clustering end--\")\n","\n","\n","    # 클러스터링 결과 시각적으로 봐보기\n","    print(\"--show image clustering result start--\")\n","    target = \"이번 생은 가주가 되겠습니다\"\n","    if clustering_type == 1:\n","        # Style Transfer로 추출한 이미지 -> kmeans 및 dists 계산 결과\n","        ct = Clustering(trained_images)\n","        folder = \"resized_images\"\n","    elif clustering_type == 2:\n","        # Style Tranfer로 추출한 이미지 -> haralick로 특징 추출 -> kmeans 및 dists 계산 결과\n","        ct = Clustering(style_image_features)\n","        folder = \"style_images\"\n","    else:\n","        # 기본 이미지 -> haralick로 특징 추출 -> kmeans 및 dists 계산 결과\n","        ct = Clustering(image_features)\n","        folder = \"resized_images\"\n","    ct.svd(svd_n)\n","    ct.kmeans_cluster(k)\n","    ct.visualize()\n","    ct.calculate_dists()\n","    ct.plotImages(target, folder)\n","    print(\"--show image clustering result end--\")\n","\n","\n","    # 결과 저장하기\n","    td.total_data[\"cluster_style_group\"] = [ct.get_close_images_idx(i, 12) for i in range(len(td.total_data))]\n","    td.total_data[\"cluster_style\"] = ct.pred"],"metadata":{"id":"fJ_GFYtej0hB","executionInfo":{"status":"ok","timestamp":1669464211045,"user_tz":-540,"elapsed":3,"user":{"displayName":"ddophi","userId":"07063622790582619538"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    do_web_crawling()\n","    story_clustering = do_tokenize_and_vectorize()\n","    do_clustering_by_story(story_clustering, k_for_total=64)\n","    do_clustering_by_style(k=32, style_train_n=256, svd_n=2, clustering_type=2)\n","    ut.make_csv(cluster_csv_filename, td.total_data)\n","    ut.make_csv(cluster_detail_csv_filename, td.cluster_details)"],"metadata":{"id":"P7dcmgocLsVQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LA9FGDP7V6tZ"},"source":["# 깃허브 관련"]},{"cell_type":"markdown","metadata":{"id":"2--jwgVwZb9Z"},"source":["취소 관련   \n","https://gmlwjd9405.github.io/2018/05/25/git-add-cancle.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVDIHfH7OcYE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668693556094,"user_tz":-540,"elapsed":3,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"e07692e6-d877-4173-9e46-9d42ff99c5d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Github/IndividualProject-webtoonRecommendation/DeveloperAcademy-NC2-WebtoonRecommendation/MachineLearning\n"]}],"source":["cd /content/drive/MyDrive/Colab\\ Notebooks/Github/IndividualProject-webtoonRecommendation/DeveloperAcademy-NC2-WebtoonRecommendation/MachineLearning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKBdElx8VoFc"},"outputs":[],"source":["!git config --global user.email \"lightcloud98@gmail.com\"\n","!git config --global user.name \"ddophi98\""]},{"cell_type":"code","source":["!git checkout machineLearning"],"metadata":{"id":"iW5wgj_XpDIT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668693584478,"user_tz":-540,"elapsed":24857,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"eb300627-bca6-4a74-e533-3a95b26e2d33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M\tMachineLearning/WentoonRecommendML.ipynb\n","Already on 'machineLearning'\n","Your branch is up to date with 'origin/machineLearning'.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fFNGHLQVE5n"},"outputs":[],"source":["!git add WentoonRecommendML.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L_-LAUZX_Cm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668345524550,"user_tz":-540,"elapsed":8,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"d29411dd-51fe-4372-de32-86b9479c9219"},"outputs":[{"output_type":"stream","name":"stdout","text":["On branch machineLearning\n","Your branch is up to date with 'origin/machineLearning'.\n","\n","Changes to be committed:\n","  (use \"git reset HEAD <file>...\" to unstage)\n","\n","\t\u001b[32mmodified:   WentoonRecommendML.ipynb\u001b[m\n","\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6GkdJmjV_PD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668693589248,"user_tz":-540,"elapsed":3498,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"03bcb712-96c3-484b-e882-ee34dff380bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[machineLearning 9defdbb] 스토리 분류 관련 코드 전체적으로 리팩토링\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite MachineLearning/WentoonRecommendML.ipynb (98%)\n"]}],"source":["!git commit -m \"스토리 분류 관련 코드 전체적으로 리팩토링\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa5WplLwYOh-"},"outputs":[],"source":["!git log"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K33gtod9VVZk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668693600165,"user_tz":-540,"elapsed":10929,"user":{"displayName":"ddophi","userId":"07063622790582619538"}},"outputId":"2b65147a-8e1e-4337-df27-5a96d1393fbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Counting objects: 1   \rCounting objects: 4, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:  25% (1/4)   \rCompressing objects:  50% (2/4)   \rCompressing objects:  75% (3/4)   \rCompressing objects: 100% (4/4)   \rCompressing objects: 100% (4/4), done.\n","Writing objects:  25% (1/4)   \rWriting objects:  50% (2/4)   \rWriting objects:  75% (3/4)   \rWriting objects: 100% (4/4)   \rWriting objects: 100% (4/4), 2.90 KiB | 494.00 KiB/s, done.\n","Total 4 (delta 2), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/ddophi98/DeveloperAcademy-NC2-WebtoonRecommendation\n","   5330e17..9defdbb  machineLearning -> machineLearning\n","Branch 'machineLearning' set up to track remote branch 'machineLearning' from 'origin'.\n"]}],"source":["!git push -u origin"]}]}